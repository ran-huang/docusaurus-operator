"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[3005],{3905:function(e,t,a){a.d(t,{Zo:function(){return s},kt:function(){return h}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var d=n.createContext({}),p=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},s=function(e){var t=p(e.components);return n.createElement(d.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,l=e.originalType,d=e.parentName,s=i(e,["components","mdxType","originalType","parentName"]),c=p(a),h=o,u=c["".concat(d,".").concat(h)]||c[h]||m[h]||l;return a?n.createElement(u,r(r({ref:t},s),{},{components:a})):n.createElement(u,r({ref:t},s))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var l=a.length,r=new Array(l);r[0]=c;var i={};for(var d in t)hasOwnProperty.call(t,d)&&(i[d]=t[d]);i.originalType=e,i.mdxType="string"==typeof e?e:o,r[1]=i;for(var p=2;p<l;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},5017:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return i},contentTitle:function(){return d},metadata:function(){return p},assets:function(){return s},toc:function(){return m},default:function(){return h}});var n=a(7462),o=a(3366),l=(a(7294),a(3905)),r=["components"],i={title:"Maintain Kubernetes Nodes that Hold the TiDB Cluster",summary:"Learn how to maintain Kubernetes nodes that hold the TiDB cluster."},d="Maintain Kubernetes Nodes that Hold the TiDB Cluster",p={unversionedId:"maintain-a-kubernetes-node",id:"maintain-a-kubernetes-node",title:"Maintain Kubernetes Nodes that Hold the TiDB Cluster",description:"TiDB is a highly available database that can run smoothly when some of the database nodes go offline. For this reason, you can safely shut down and maintain the Kubernetes nodes at the bottom layer without influencing TiDB's service. Specifically, you need to adopt various maintenance strategies when handling PD, TiKV, and TiDB Pods because of their different characteristics.",source:"@site/docs/maintain-a-kubernetes-node.md",sourceDirName:".",slug:"/maintain-a-kubernetes-node",permalink:"/docusaurus-operator/maintain-a-kubernetes-node",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/maintain-a-kubernetes-node.md",tags:[],version:"current",frontMatter:{title:"Maintain Kubernetes Nodes that Hold the TiDB Cluster",summary:"Learn how to maintain Kubernetes nodes that hold the TiDB cluster."},sidebar:"mySidebar",previous:{title:"Maintain Different TiDB Clusters Separately Using Multiple Sets of TiDB Operator",permalink:"/docusaurus-operator/deploy-multiple-tidb-operator"},next:{title:"Migrate from Helm 2 to Helm 3",permalink:"/docusaurus-operator/migrate-to-helm3"}},s={},m=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Maintain a node that can be recovered shortly",id:"maintain-a-node-that-can-be-recovered-shortly",level:2},{value:"Maintain a node that cannot be recovered shortly",id:"maintain-a-node-that-cannot-be-recovered-shortly",level:2},{value:"Reschedule PD Pods",id:"reschedule-pd-pods",level:2},{value:"If the node storage can be automatically migrated",id:"if-the-node-storage-can-be-automatically-migrated",level:3},{value:"If the node storage cannot be automatically migrated",id:"if-the-node-storage-cannot-be-automatically-migrated",level:3},{value:"Reschedule TiKV Pods",id:"reschedule-tikv-pods",level:2},{value:"If the node storage can be automatically migrated",id:"if-the-node-storage-can-be-automatically-migrated-1",level:3},{value:"If the node storage cannot be automatically migrated",id:"if-the-node-storage-cannot-be-automatically-migrated-1",level:3},{value:"Transfer PD Leader",id:"transfer-pd-leader",level:2},{value:"Evict TiKV Region Leader",id:"evict-tikv-region-leader",level:2}],c={toc:m};function h(e){var t=e.components,a=(0,o.Z)(e,r);return(0,l.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"maintain-kubernetes-nodes-that-hold-the-tidb-cluster"},"Maintain Kubernetes Nodes that Hold the TiDB Cluster"),(0,l.kt)("p",null,"TiDB is a highly available database that can run smoothly when some of the database nodes go offline. For this reason, you can safely shut down and maintain the Kubernetes nodes at the bottom layer without influencing TiDB's service. Specifically, you need to adopt various maintenance strategies when handling PD, TiKV, and TiDB Pods because of their different characteristics."),(0,l.kt)("p",null,"This document introduces how to perform a temporary or long-term maintenance task for the Kubernetes nodes."),(0,l.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://kubernetes.io/docs/tasks/tools/install-kubectl/"},(0,l.kt)("inlineCode",{parentName:"a"},"kubectl"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"/docusaurus-operator/use-tkctl"},(0,l.kt)("inlineCode",{parentName:"a"},"tkctl"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://stedolan.github.io/jq/download/"},(0,l.kt)("inlineCode",{parentName:"a"},"jq")))),(0,l.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,l.kt)("div",{parentName:"div",className:"admonition-heading"},(0,l.kt)("h5",{parentName:"div"},(0,l.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,l.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,l.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,l.kt)("div",{parentName:"div",className:"admonition-content"},(0,l.kt)("p",{parentName:"div"},"Before you maintain a node, you need to make sure that the remaining resources in the Kubernetes cluster are enough for running the TiDB cluster."))),(0,l.kt)("h2",{id:"maintain-a-node-that-can-be-recovered-shortly"},"Maintain a node that can be recovered shortly"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mark the node to be maintained as non-schedulable to ensure that no new Pod is scheduled to it:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl cordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check whether there is any TiKV Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep tikv\n")),(0,l.kt)("p",{parentName:"li"},"If any TiKV Pod is found, for each TiKV Pod, perform the following operations:"),(0,l.kt)("ol",{parentName:"li"},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("a",{parentName:"p",href:"#evict-tikv-region-leader"},"Evict the TiKV Region Leader")," to another Pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Increase the maximum offline duration for TiKV Pods by configuring ",(0,l.kt)("inlineCode",{parentName:"p"},"max-store-down-time")," of PD. After you maintain and recover the Kubernetes node within that duration, all TiKV Pods on that node will be automatically recovered."),(0,l.kt)("p",{parentName:"li"},"The following example shows how to set ",(0,l.kt)("inlineCode",{parentName:"p"},"max-store-down-time")," to ",(0,l.kt)("inlineCode",{parentName:"p"},"60m"),". You can set it to any reasonable value."),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl config set max-store-down-time 60m\n"))))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check whether there is any PD Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep pd\n")),(0,l.kt)("p",{parentName:"li"},"If any PD Pod is found, for each PD Pod, ",(0,l.kt)("a",{parentName:"p",href:"#transfer-pd-leader"},"transfer the PD leader")," to other Pods.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the node to be maintained no longer has any TiKV Pod or PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Migrate all Pods on the node to be maintained to other nodes:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl drain ${node_name} --ignore-daemonsets\n")),(0,l.kt)("p",{parentName:"li"},"After running this command, all Pods on this node are automatically migrated to another available node.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the node to be maintained no longer has any TiKV, TiDB, or PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"When the maintenance is completed, after you recover the node, make sure that the node is in a healthy state:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch kubectl get node ${node_name}\n")),(0,l.kt)("p",{parentName:"li"},"After the node goes into the ",(0,l.kt)("inlineCode",{parentName:"p"},"Ready")," state, proceed with the following operations.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Lift the scheduling restriction on the node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl uncordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that all Pods are running normally:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name}\n")),(0,l.kt)("p",{parentName:"li"},"When all Pods are running normally, you have successfully finished the maintenance task."))),(0,l.kt)("h2",{id:"maintain-a-node-that-cannot-be-recovered-shortly"},"Maintain a node that cannot be recovered shortly"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check whether there is any TiKV Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep tikv\n")),(0,l.kt)("p",{parentName:"li"},"If any TiKV Pod is found, for each TiKV Pod, ",(0,l.kt)("a",{parentName:"p",href:"#reschedule-tikv-pods"},"reschedule the TiKV Pod")," to another node.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check whether there is any PD Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep pd\n")),(0,l.kt)("p",{parentName:"li"},"If any PD Pod is found, for each PD Pod, ",(0,l.kt)("a",{parentName:"p",href:"#reschedule-pd-pods"},"reschedule the PD Pod")," to another node.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the node to be maintained no longer has any TiKV Pod or PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Migrate all Pods on the node to be maintained to other nodes:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl drain ${node_name} --ignore-daemonsets\n")),(0,l.kt)("p",{parentName:"li"},"After running this command, all Pods on this node are automatically migrated to another available node.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the node to be maintained no longer has any TiKV, TiDB, or PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"(Optional) If the node will be offline for a long time, it is recommended to delete the node from your Kubernetes cluster:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete node ${node_name}\n")))),(0,l.kt)("h2",{id:"reschedule-pd-pods"},"Reschedule PD Pods"),(0,l.kt)("p",null,"If a node will be offline for a long time, to minimize the impact on your application, you can reschedule the PD Pods on this node to other nodes in advance."),(0,l.kt)("h3",{id:"if-the-node-storage-can-be-automatically-migrated"},"If the node storage can be automatically migrated"),(0,l.kt)("p",null,"If the node storage can be automatically migrated (such as EBS), to reschedule a PD Pod, you do not need to delete the PD member. You only need to transfer the PD Leader to another Pod and delete the old Pod."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mark the node to be maintained as non-schedulable to ensure that no new Pod is scheduled to it:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl cordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the PD Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep pd\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("a",{parentName:"p",href:"#transfer-pd-leader"},"Transfer the PD Leader")," to another Pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the old PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pod ${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the PD Pod is successfully scheduled to another node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch kubectl -n ${namespace} get pod -o wide\n")))),(0,l.kt)("h3",{id:"if-the-node-storage-cannot-be-automatically-migrated"},"If the node storage cannot be automatically migrated"),(0,l.kt)("p",null,"If the node storage cannot be automatically migrated (such as local storage), to reschedule a PD Pod, you need to delete the PD member."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mark the node to be maintained as non-schedulable to ensure that no new Pod is scheduled to it:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl cordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the PD Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep pd\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("a",{parentName:"p",href:"#transfer-pd-leader"},"Transfer the PD Leader")," to another Pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Take the PD Pod offline:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl member delete name ${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the PD member is deleted:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl member\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Unbind the PD Pod with the local disk on the node."),(0,l.kt)("ol",{parentName:"li"},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the ",(0,l.kt)("inlineCode",{parentName:"p"},"PersistentVolumeClaim")," used by the Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl -n ${namespace} get pvc -l tidb.pingcap.com/pod-name=${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the ",(0,l.kt)("inlineCode",{parentName:"p"},"PersistentVolumeClaim"),":"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pvc ${pvc_name} --wait=false\n"))))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the old PD Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pod ${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the PD Pod is successfully scheduled to another node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch kubectl -n ${namespace} get pod -o wide\n")))),(0,l.kt)("h2",{id:"reschedule-tikv-pods"},"Reschedule TiKV Pods"),(0,l.kt)("p",null,"If a node will be offline for a long time, to minimize the impact on your application, you can reschedule the TiKV Pods on this node to other nodes in advance."),(0,l.kt)("h3",{id:"if-the-node-storage-can-be-automatically-migrated-1"},"If the node storage can be automatically migrated"),(0,l.kt)("p",null,"If the node storage can be automatically migrated (such as EBS), to reschedule a TiKV Pod, you do not need to delete the whole TiKV store. You only need to evict the TiKV Region Leader to another Pod and delete the old Pod."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mark the node to be maintained as non-schedulable to ensure that no new Pod is scheduled to it:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl cordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the TiKV Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep tikv\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("a",{parentName:"p",href:"#evict-tikv-region-leader"},"Evict the TiKV Region Leader")," to another Pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the old TiKV Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pod ${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the TiKV Pod is successfully scheduled to another node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch kubectl -n ${namespace} get pod -o wide\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Remove evict-leader-scheduler, and wait for the Region Leader to automatically schedule back:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl scheduler remove evict-leader-scheduler-${ID}\n")))),(0,l.kt)("h3",{id:"if-the-node-storage-cannot-be-automatically-migrated-1"},"If the node storage cannot be automatically migrated"),(0,l.kt)("p",null,"If the node storage cannot be automatically migrated (such as local storage), to reschedule a TiKV Pod, you need to delete the whole TiKV store."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Mark the node to be maintained as non-schedulable to ensure that no new Pod is scheduled to it:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl cordon ${node_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the TiKV Pod on the node to be maintained:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pod --all-namespaces -o wide | grep ${node_name} | grep tikv\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("a",{parentName:"p",href:"#evict-tikv-region-leader"},"Evict the TiKV Region Leader")," to another Pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Take the TiKV Pod offline."),(0,l.kt)("blockquote",{parentName:"li"},(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"},"Note:")),(0,l.kt)("p",{parentName:"blockquote"},"Before you take the TiKV Pod offline, make sure that the remaining TiKV Pods are not fewer than the TiKV replica number set in PD configuration (",(0,l.kt)("inlineCode",{parentName:"p"},"max-replicas"),", 3 by default). If the remaining TiKV Pods are not enough, scale out TiKV Pods before you take the TiKV Pod offline.")),(0,l.kt)("ol",{parentName:"li"},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check ",(0,l.kt)("inlineCode",{parentName:"p"},"store-id")," of the TiKV Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl get tc ${cluster_name} -ojson | jq ".status.tikv.stores | .[] | select ( .podName == \\"${pod_name}\\" ) | .id"\n'))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Take the Pod offline:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl store delete ${ID}\n"))))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Wait for the store status (",(0,l.kt)("inlineCode",{parentName:"p"},"state_name"),") to become ",(0,l.kt)("inlineCode",{parentName:"p"},"Tombstone"),":"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch pd-ctl store ${ID}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Unbind the TiKV Pod with the local disk on the node."),(0,l.kt)("ol",{parentName:"li"},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the ",(0,l.kt)("inlineCode",{parentName:"p"},"PersistentVolumeClaim")," used by the Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl -n ${namespace} get pvc -l tidb.pingcap.com/pod-name=${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the ",(0,l.kt)("inlineCode",{parentName:"p"},"PersistentVolumeClaim"),":"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pvc ${pvc_name} --wait=false\n"))))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Delete the old TiKV Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete -n ${namespace} pod ${pod_name}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that the TiKV Pod is successfully scheduled to another node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"watch kubectl -n ${namespace} get pod -o wide\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Remove evict-leader-scheduler, and wait for the Region Leader to automatically schedule back:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl scheduler remove evict-leader-scheduler-${ID}\n")))),(0,l.kt)("h2",{id:"transfer-pd-leader"},"Transfer PD Leader"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check the PD Leader:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl member leader show\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"If the Leader Pod is on the node to be maintained, you need to transfer the PD Leader to a Pod on another node:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl member leader transfer ${pod_name}\n")),(0,l.kt)("p",{parentName:"li"},(0,l.kt)("inlineCode",{parentName:"p"},"${pod_name}")," is the name of the PD Pod on another node."))),(0,l.kt)("h2",{id:"evict-tikv-region-leader"},"Evict TiKV Region Leader"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Check ",(0,l.kt)("inlineCode",{parentName:"p"},"store-id")," of the TiKV Pod:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl get tc ${cluster_name} -ojson | jq ".status.tikv.stores | .[] | select ( .podName == \\"${pod_name}\\" ) | .id"\n'))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Evict the Region Leader:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},"pd-ctl scheduler add evict-leader-scheduler ${ID}\n"))),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},"Confirm that all Region Leaders are migrated:"),(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl get tc ${cluster_name} -ojson | jq ".status.tikv.stores | .[] | select ( .podName == \\"${pod_name}\\" ) | .leaderCount"\n')))))}h.isMDXComponent=!0}}]);