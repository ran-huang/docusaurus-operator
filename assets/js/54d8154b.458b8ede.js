"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[2618],{3905:function(e,t,n){n.d(t,{Zo:function(){return m},kt:function(){return u}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},m=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,p=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),c=s(n),u=i,k=c["".concat(p,".").concat(u)]||c[u]||d[u]||r;return n?a.createElement(k,l(l({ref:t},m),{},{components:n})):a.createElement(k,l({ref:t},m))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,l=new Array(r);l[0]=c;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:i,l[1]=o;for(var s=2;s<r;s++)l[s]=n[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},2361:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return o},contentTitle:function(){return p},metadata:function(){return s},assets:function(){return m},toc:function(){return d},default:function(){return u}});var a=n(7462),i=n(3366),r=(n(7294),n(3905)),l=["components"],o={title:"Deploy TiDB Binlog",summary:"Learn how to deploy TiDB Binlog for a TiDB cluster in Kubernetes."},p="Deploy TiDB Binlog",s={unversionedId:"deploy-tidb-binlog",id:"deploy-tidb-binlog",title:"Deploy TiDB Binlog",description:"This document describes how to maintain TiDB Binlog of a TiDB cluster in Kubernetes.",source:"@site/docs/deploy-tidb-binlog.md",sourceDirName:".",slug:"/deploy-tidb-binlog",permalink:"/deploy-tidb-binlog",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/deploy-tidb-binlog.md",tags:[],version:"current",frontMatter:{title:"Deploy TiDB Binlog",summary:"Learn how to deploy TiDB Binlog for a TiDB cluster in Kubernetes."},sidebar:"mySidebar",previous:{title:"Deploy TiCDC in Kubernetes",permalink:"/deploy-ticdc"},next:{title:"Import Data",permalink:"/restore-data-using-tidb-lightning"}},m={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Deploy TiDB Binlog in a TiDB cluster",id:"deploy-tidb-binlog-in-a-tidb-cluster",level:2},{value:"Deploy Pump",id:"deploy-pump",level:3},{value:"Deploy Drainer",id:"deploy-drainer",level:2},{value:"Enable TLS",id:"enable-tls",level:2},{value:"Enable TLS between TiDB components",id:"enable-tls-between-tidb-components",level:3},{value:"Enable TLS between Drainer and the downstream database",id:"enable-tls-between-drainer-and-the-downstream-database",level:3},{value:"Remove Pump/Drainer nodes",id:"remove-pumpdrainer-nodes",level:2},{value:"Scale in Pump nodes",id:"scale-in-pump-nodes",level:3},{value:"Remove Pump nodes completely",id:"remove-pump-nodes-completely",level:3},{value:"Remove Drainer nodes",id:"remove-drainer-nodes",level:3}],c={toc:d};function u(e){var t=e.components,n=(0,i.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"deploy-tidb-binlog"},"Deploy TiDB Binlog"),(0,r.kt)("p",null,"This document describes how to maintain ",(0,r.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/tidb-binlog/tidb-binlog-overview/"},"TiDB Binlog")," of a TiDB cluster in Kubernetes."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/deploy-tidb-operator"},"Deploy TiDB Operator"),";"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/tidb-toolkit#use-helm"},"Install Helm")," and configure it with the official PingCAP chart.")),(0,r.kt)("h2",{id:"deploy-tidb-binlog-in-a-tidb-cluster"},"Deploy TiDB Binlog in a TiDB cluster"),(0,r.kt)("p",null,"TiDB Binlog is disabled in the TiDB cluster by default. To create a TiDB cluster with TiDB Binlog enabled, or enable TiDB Binlog in an existing TiDB cluster, take the following steps."),(0,r.kt)("h3",{id:"deploy-pump"},"Deploy Pump"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"TidbCluster")," CR file to add the Pump configuration."),(0,r.kt)("p",{parentName:"li"},"For example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  pump:\n    baseImage: pingcap/tidb-binlog\n    version: v5.3.0\n    replicas: 1\n    storageClassName: local-storage\n    requests:\n      storage: 30Gi\n    schedulerName: default-scheduler\n    config:\n      addr: 0.0.0.0:8250\n      gc: 7\n      heartbeat-interval: 2\n")),(0,r.kt)("p",{parentName:"li"},"Since v1.1.6, TiDB Operator supports passing raw TOML configuration to the component:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  ...\n  pump:\n    baseImage: pingcap/tidb-binlog\n    version: v5.3.0\n    replicas: 1\n    storageClassName: local-storage\n    requests:\n      storage: 30Gi\n    schedulerName: default-scheduler\n    config: |\n      addr = "0.0.0.0:8250"\n      gc = 7\n      heartbeat-interval = 2\n')),(0,r.kt)("p",{parentName:"li"},"Edit ",(0,r.kt)("inlineCode",{parentName:"p"},"version"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"replicas"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"storageClassName"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"requests.storage")," according to your cluster."),(0,r.kt)("p",{parentName:"li"},"To deploy Enterprise Edition of Pump, edit the YAML file above to set ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.pump.baseImage")," to the enterprise image (",(0,r.kt)("inlineCode",{parentName:"p"},"pingcap/tidb-binlog-enterprise"),")."),(0,r.kt)("p",{parentName:"li"},"For example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  pump:\n    baseImage: pingcap/tidb-binlog-enterprise\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Set affinity and anti-affinity for TiDB and Pump."),(0,r.kt)("p",{parentName:"li"},"If you enable TiDB Binlog in the production environment, it is recommended to set affinity and anti-affinity for TiDB and the Pump component; if you enable TiDB Binlog in a test environment on the internal network, you can skip this step."),(0,r.kt)("p",{parentName:"li"},"By default, the affinity of TiDB and Pump is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"{}"),". Currently, each TiDB instance does not have a corresponding Pump instance by default. When TiDB Binlog is enabled, if Pump and TiDB are separately deployed and network isolation occurs, and ",(0,r.kt)("inlineCode",{parentName:"p"},"ignore-error")," is enabled in TiDB components, TiDB loses binlogs."),(0,r.kt)("p",{parentName:"li"},"In this situation, it is recommended to deploy a TiDB instance and a Pump instance on the same node using the affinity feature, and to split Pump instances on different nodes using the anti-affinity feature. For each node, only one Pump instance is required. The steps are as follows:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Configure ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.tidb.affinity")," as follows:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tidb:\n    affinity:\n      podAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchExpressions:\n              - key: "app.kubernetes.io/component"\n                operator: In\n                values:\n                - "pump"\n              - key: "app.kubernetes.io/managed-by"\n                operator: In\n                values:\n                - "tidb-operator"\n              - key: "app.kubernetes.io/name"\n                operator: In\n                values:\n                - "tidb-cluster"\n              - key: "app.kubernetes.io/instance"\n                operator: In\n                values:\n                - ${cluster_name}\n            topologyKey: kubernetes.io/hostname\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Configure ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.pump.affinity")," as follows:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  pump:\n    affinity:\n      podAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchExpressions:\n              - key: "app.kubernetes.io/component"\n                operator: In\n                values:\n                - "tidb"\n              - key: "app.kubernetes.io/managed-by"\n                operator: In\n                values:\n                - "tidb-operator"\n              - key: "app.kubernetes.io/name"\n                operator: In\n                values:\n                - "tidb-cluster"\n              - key: "app.kubernetes.io/instance"\n                operator: In\n                values:\n                - ${cluster_name}\n            topologyKey: kubernetes.io/hostname\n      podAntiAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchExpressions:\n              - key: "app.kubernetes.io/component"\n                operator: In\n                values:\n                - "pump"\n              - key: "app.kubernetes.io/managed-by"\n                operator: In\n                values:\n                - "tidb-operator"\n              - key: "app.kubernetes.io/name"\n                operator: In\n                values:\n                - "tidb-cluster"\n              - key: "app.kubernetes.io/instance"\n                operator: In\n                values:\n                - ${cluster_name}\n            topologyKey: kubernetes.io/hostname\n')))),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"If you update the affinity configuration of the TiDB components, it will cause rolling updates of the TiDB components in the cluster.")))),(0,r.kt)("h2",{id:"deploy-drainer"},"Deploy Drainer"),(0,r.kt)("p",null,"To deploy multiple drainers using the ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-drainer")," Helm chart for a TiDB cluster, take the following steps:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Make sure that the PingCAP Helm repository is up to date:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"helm repo update\n")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"helm search repo tidb-drainer -l\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Get the default ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," file to facilitate customization:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"helm inspect values pingcap/tidb-drainer --version=${chart_version} > values.yaml\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Modify the ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," file to specify the source TiDB cluster and the downstream database of the drainer. Here is an example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'clusterName: example-tidb\nclusterVersion: v5.3.0\nbaseImage:pingcap/tidb-binlog\nstorageClassName: local-storage\nstorage: 10Gi\ninitialCommitTs: "-1"\nconfig: |\n  detect-interval = 10\n  [syncer]\n  worker-count = 16\n  txn-batch = 20\n  disable-dispatch = false\n  ignore-schemas = "INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql"\n  safe-mode = false\n  db-type = "tidb"\n  [syncer.to]\n  host = "downstream-tidb"\n  user = "root"\n  password = ""\n  port = 4000\n')),(0,r.kt)("p",{parentName:"li"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"clusterName")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"clusterVersion")," must match the desired source TiDB cluster."),(0,r.kt)("p",{parentName:"li"},"The ",(0,r.kt)("inlineCode",{parentName:"p"},"initialCommitTs")," is the starting commit timestamp of data replication when Drainer has no checkpoint. The value must be set as a string type, such as ",(0,r.kt)("inlineCode",{parentName:"p"},'"424364429251444742"'),"."),(0,r.kt)("p",{parentName:"li"},"For complete configuration details, refer to ",(0,r.kt)("a",{parentName:"p",href:"/configure-tidb-binlog-drainer"},"TiDB Binlog Drainer Configurations in Kubernetes"),"."),(0,r.kt)("p",{parentName:"li"},"To deploy Enterprise Edition of Drainer, edit the YAML file above to set ",(0,r.kt)("inlineCode",{parentName:"p"},"baseImage")," to the enterprise image (",(0,r.kt)("inlineCode",{parentName:"p"},"pingcap/tidb-binlog-enterprise"),")."),(0,r.kt)("p",{parentName:"li"},"For example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"...\nclusterVersion: v5.3.0\nbaseImage: pingcap/tidb-binlog-enterprise\n...\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Deploy Drainer:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"helm install ${release_name} pingcap/tidb-drainer --namespace=${namespace} --version=${chart_version} -f values.yaml\n")),(0,r.kt)("p",{parentName:"li"},"If the server does not have an external network, refer to ",(0,r.kt)("a",{parentName:"p",href:"/deploy-on-general-kubernetes#deploy-the-tidb-cluster"},"deploy the TiDB cluster")," to download the required Docker image on the machine with an external network and upload it to the server."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"This chart must be installed to the same namespace as the source TiDB cluster.")))),(0,r.kt)("h2",{id:"enable-tls"},"Enable TLS"),(0,r.kt)("h3",{id:"enable-tls-between-tidb-components"},"Enable TLS between TiDB components"),(0,r.kt)("p",null,"If you want to enable TLS for the TiDB cluster and TiDB Binlog, refer to ",(0,r.kt)("a",{parentName:"p",href:"/enable-tls-between-components"},"Enable TLS between Components"),"."),(0,r.kt)("p",null,"After you have created a secret and started a TiDB cluster with Pump, edit the ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," file to set the ",(0,r.kt)("inlineCode",{parentName:"p"},"tlsCluster.enabled")," value to ",(0,r.kt)("inlineCode",{parentName:"p"},"true"),", and configure the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"certAllowedCN"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"...\ntlsCluster:\n  enabled: true\n  # certAllowedCN:\n  #  - TiDB\n...\n")),(0,r.kt)("h3",{id:"enable-tls-between-drainer-and-the-downstream-database"},"Enable TLS between Drainer and the downstream database"),(0,r.kt)("p",null,"If you set the downstream database of ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-drainer")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"mysql/tidb"),", and if you want to enable TLS between Drainer and the downstream database, take the following steps."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a secret that contains the TLS information of the downstream database."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create secret generic ${downstream_database_secret_name} --namespace=${namespace} --from-file=tls.crt=client.pem --from-file=tls.key=client-key.pem --from-file=ca.crt=ca.pem\n")),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"tidb-drainer")," saves the checkpoint in the downstream database by default, so you only need to configure ",(0,r.kt)("inlineCode",{parentName:"p"},"tlsSyncer.tlsClientSecretName")," and the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"cerAllowedCN"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"tlsSyncer:\n  tlsClientSecretName: ${downstream_database_secret_name}\n  # certAllowedCN:\n  #  - TiDB\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"To save the checkpoint of ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-drainer")," to ",(0,r.kt)("strong",{parentName:"p"},"other databases that have enabled TLS"),", create a secret that contains the TLS information of the checkpoint database:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create secret generic ${checkpoint_tidb_client_secret} --namespace=${namespace} --from-file=tls.crt=client.pem --from-file=tls.key=client-key.pem --from-file=ca.crt=ca.pem\n")),(0,r.kt)("p",{parentName:"li"},"Edit the ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," file to set the ",(0,r.kt)("inlineCode",{parentName:"p"},"tlsSyncer.checkpoint.tlsClientSecretName")," value to ",(0,r.kt)("inlineCode",{parentName:"p"},"${checkpoint_tidb_client_secret}"),", and configure the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"certAllowedCN"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"...\ntlsSyncer: {}\n  tlsClientSecretName: ${downstream_database_secret_name}\n  # certAllowedCN:\n  #  - TiDB\n  checkpoint:\n    tlsClientSecretName: ${checkpoint_tidb_client_secret}\n    # certAllowedCN:\n    #  - TiDB\n...\n")))),(0,r.kt)("h2",{id:"remove-pumpdrainer-nodes"},"Remove Pump/Drainer nodes"),(0,r.kt)("p",null,"For details on how to maintain the node state of the TiDB Binlog cluster, refer to ",(0,r.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/maintain-tidb-binlog-cluster#starting-and-exiting-a-pump-or-drainer-process"},"Starting and exiting a Pump or Drainer process"),"."),(0,r.kt)("p",null,"If you want to remove the TiDB Binlog component completely, it is recommended that you first remove Pump nodes and then remove Drainer nodes."),(0,r.kt)("p",null,"If TLS is enabled for the TiDB Binlog component to be removed, write the following content into ",(0,r.kt)("inlineCode",{parentName:"p"},"binlog.yaml")," and execute ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl apply -f binlog.yaml")," to start a Pod that is mounted with the TLS file and the ",(0,r.kt)("inlineCode",{parentName:"p"},"binlogctl")," tool."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nmetadata:\n  name: binlogctl\nspec:\n  containers:\n  - name: binlogctl\n    image: pingcap/tidb-binlog:${tidb_version}\n    command: ['/bin/sh']\n    stdin: true\n    stdinOnce: true\n    tty: true\n    volumeMounts:\n      - name: binlog-tls\n        mountPath: /etc/binlog-tls\n  volumes:\n    - name: binlog-tls\n      secret:\n        secretName: ${cluster_name}-cluster-client-secret\n")),(0,r.kt)("h3",{id:"scale-in-pump-nodes"},"Scale in Pump nodes"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Scale in Pump Pods:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl patch tc ${cluster_name} -n ${namespace} --type merge -p \'{"spec":{"pump":{"replicas": ${pump_replicas}}}}\'\n')),(0,r.kt)("p",{parentName:"li"},"In the command above, ",(0,r.kt)("inlineCode",{parentName:"p"},"${pump_replicas}")," is the desired number of Pump Pods after the scaling."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"Do not scale in Pump nodes to 0. Otherwise, ",(0,r.kt)("a",{parentName:"p",href:"#remove-pump-nodes-completely"},"Pump nodes are removed completely"),"."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Wait for the Pump Pods to automatically be taken offline and deleted. Run the following command to observe the Pod status:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"watch kubectl get po ${cluster_name} -n ${namespace}\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"(Optional) Force Pump to go offline:"),(0,r.kt)("p",{parentName:"li"},"If the offline operation fails, that is, the Pump Pods are not deleted for a long time, you can forcibly mark Pump as ",(0,r.kt)("inlineCode",{parentName:"p"},"offline"),"."),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is not enabled for Pump, mark Pump as ",(0,r.kt)("inlineCode",{parentName:"p"},"offline"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl run update-pump-${ordinal_id} --image=pingcap/tidb-binlog:${tidb_version} --namespace=${namespace} --restart=OnFailure -- /binlogctl -pd-urls=http://${cluster_name}-pd:2379 -cmd update-pump -node-id ${cluster_name}-pump-${ordinal_id}:8250 --state offline\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is enabled for Pump, mark Pump as ",(0,r.kt)("inlineCode",{parentName:"p"},"offline")," using the previously started Pod:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl exec binlogctl -n ${namespace} -- /binlogctl -pd-urls=https://${cluster_name}-pd:2379 -cmd update-pump -node-id ${cluster_name}-pump-${ordinal_id}:8250 --state offline -ssl-ca "/etc/binlog-tls/ca.crt" -ssl-cert "/etc/binlog-tls/tls.crt" -ssl-key "/etc/binlog-tls/tls.key"\n')))))),(0,r.kt)("h3",{id:"remove-pump-nodes-completely"},"Remove Pump nodes completely"),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("ul",{parentName:"div"},(0,r.kt)("li",{parentName:"ul"},"Before performing the following steps, you need to have at least one Pump node in the cluster. If you have scaled in Pump nodes to ",(0,r.kt)("inlineCode",{parentName:"li"},"0"),", you need to scale out Pump at least to ",(0,r.kt)("inlineCode",{parentName:"li"},"1")," node before you perform the removing operation in this section."),(0,r.kt)("li",{parentName:"ul"},"To scale out the Pump to ",(0,r.kt)("inlineCode",{parentName:"li"},"1"),", execute ",(0,r.kt)("inlineCode",{parentName:"li"},'kubectl patch tc ${tidb-cluster} -n ${namespace} --type merge -p \'{"spec":{"pump":{"replicas": 1}}}\''),".")))),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Before removing Pump nodes, execute ",(0,r.kt)("inlineCode",{parentName:"p"},'kubectl patch tc ${cluster_name} -n ${namespace} --type merge -p \'{"spec":{"tidb":{"binlogEnabled": false}}}\''),". After the TiDB Pods are rolling updated, you can remove the Pump nodes."),(0,r.kt)("p",{parentName:"li"},"If you directly remove Pump nodes, it might cause TiDB failure because TiDB has no Pump nodes to write into.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Refer to ",(0,r.kt)("a",{parentName:"p",href:"#scale-in-pump-nodes"},"Scale in Pump")," to scale in Pump to ",(0,r.kt)("inlineCode",{parentName:"p"},"0"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Execute ",(0,r.kt)("inlineCode",{parentName:"p"},'kubectl patch tc ${cluster_name} -n ${namespace} --type json -p \'[{"op":"remove", "path":"/spec/pump"}]\'')," to delete all configuration items of ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.pump"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Execute ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl delete sts ${cluster_name}-pump -n ${namespace}")," to delete the StatefulSet resources of Pump.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"View PVCs used by the Pump cluster by executing ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get pvc -n ${namespace} -l app.kubernetes.io/component=pump"),". Then delete all the PVC resources of Pump by executing ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl delete pvc -l app.kubernetes.io/component=pump -n ${namespace}"),"."))),(0,r.kt)("h3",{id:"remove-drainer-nodes"},"Remove Drainer nodes"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Take Drainer nodes offline:"),(0,r.kt)("p",{parentName:"li"},"In the following commands, ",(0,r.kt)("inlineCode",{parentName:"p"},"${drainer_node_id}")," is the node ID of the Drainer node to be taken offline. If you have configured ",(0,r.kt)("inlineCode",{parentName:"p"},"drainerName")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," of Helm, the value of ",(0,r.kt)("inlineCode",{parentName:"p"},"${drainer_node_id}")," is ",(0,r.kt)("inlineCode",{parentName:"p"},"${drainer_name}-0"),"; otherwise, the value of ",(0,r.kt)("inlineCode",{parentName:"p"},"${drainer_node_id}")," is ",(0,r.kt)("inlineCode",{parentName:"p"},"${cluster_name}-${release_name}-drainer-0"),"."),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is not enabled for Drainer, create a Pod to take Drainer offline:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl run offline-drainer-0 --image=pingcap/tidb-binlog:${tidb_version} --namespace=${namespace} --restart=OnFailure -- /binlogctl -pd-urls=http://${cluster_name}-pd:2379 -cmd offline-drainer -node-id ${drainer_node_id}:8249\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is enabled for Drainer, use the previously started Pod to take Drainer offline:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl exec binlogctl -n ${namespace} -- /binlogctl -pd-urls "https://${cluster_name}-pd:2379" -cmd offline-drainer -node-id ${drainer_node_id}:8249 -ssl-ca "/etc/binlog-tls/ca.crt" -ssl-cert "/etc/binlog-tls/tls.crt" -ssl-key "/etc/binlog-tls/tls.key"\n')))),(0,r.kt)("p",{parentName:"li"},"View the log of Drainer by executing the following command:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl logs -f -n ${namespace} ${drainer_node_id}\n")),(0,r.kt)("p",{parentName:"li"},"If ",(0,r.kt)("inlineCode",{parentName:"p"},"drainer offline, please delete my pod")," is output, this node is successfully taken offline.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Delete the corresponding Drainer Pod:"),(0,r.kt)("p",{parentName:"li"},"Execute ",(0,r.kt)("inlineCode",{parentName:"p"},"helm uninstall ${release_name} -n ${namespace}")," to delete the Drainer Pod."),(0,r.kt)("p",{parentName:"li"},"If you no longer need Drainer, execute ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl delete pvc data-${drainer_node_id} -n ${namespace}")," to delete the PVC resources of Drainer.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"(Optional) Force Drainer to go offline:"),(0,r.kt)("p",{parentName:"li"},"If the offline operation fails, the Drainer Pod will not output ",(0,r.kt)("inlineCode",{parentName:"p"},"drainer offline, please delete my pod"),". At this time, you can force Drainer to go offline, that is, taking Step 2 to delete the Drainer Pod and mark Drainer as ",(0,r.kt)("inlineCode",{parentName:"p"},"offline"),"."),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is not enabled for Drainer, mark Drainer as ",(0,r.kt)("inlineCode",{parentName:"p"},"offline"),":"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl run update-drainer-${ordinal_id} --image=pingcap/tidb-binlog:${tidb_version} --namespace=${namespace} --restart=OnFailure -- /binlogctl -pd-urls=http://${cluster_name}-pd:2379 -cmd update-drainer -node-id ${drainer_node_id}:8249 --state offline\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If TLS is enabled for Drainer, use the previously started Pod to take Drainer offline:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl exec binlogctl -n ${namespace} -- /binlogctl -pd-urls=https://${cluster_name}-pd:2379 -cmd update-drainer -node-id ${drainer_node_id}:8249 --state offline -ssl-ca "/etc/binlog-tls/ca.crt" -ssl-cert "/etc/binlog-tls/tls.crt" -ssl-key "/etc/binlog-tls/tls.key"\n')))))))}u.isMDXComponent=!0}}]);