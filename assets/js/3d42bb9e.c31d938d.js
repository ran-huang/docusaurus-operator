"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[2215],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var o=a.createContext({}),c=function(e){var t=a.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(o.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,s=e.originalType,o=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=c(n),d=r,h=m["".concat(o,".").concat(d)]||m[d]||u[d]||s;return n?a.createElement(h,l(l({ref:t},p),{},{components:n})):a.createElement(h,l({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=n.length,l=new Array(s);l[0]=m;var i={};for(var o in t)hasOwnProperty.call(t,o)&&(i[o]=t[o]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var c=2;c<s;c++)l[c]=n[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},8827:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return o},metadata:function(){return c},assets:function(){return p},toc:function(){return u},default:function(){return d}});var a=n(7462),r=n(3366),s=(n(7294),n(3905)),l=["components"],i={title:"Deploy a TiDB Cluster across Multiple Kubernetes Clusters",summary:"Learn how to deploy a TiDB cluster across multiple Kubernetes clusters."},o=void 0,c={unversionedId:"deploy-tidb-cluster-across-multiple-kubernetes",id:"deploy-tidb-cluster-across-multiple-kubernetes",title:"Deploy a TiDB Cluster across Multiple Kubernetes Clusters",description:"This is still an experimental feature. It is NOT recommended that you use it in the production environment.",source:"@site/docs/deploy-tidb-cluster-across-multiple-kubernetes.md",sourceDirName:".",slug:"/deploy-tidb-cluster-across-multiple-kubernetes",permalink:"/deploy-tidb-cluster-across-multiple-kubernetes",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/deploy-tidb-cluster-across-multiple-kubernetes.md",tags:[],version:"current",frontMatter:{title:"Deploy a TiDB Cluster across Multiple Kubernetes Clusters",summary:"Learn how to deploy a TiDB cluster across multiple Kubernetes clusters."},sidebar:"mySidebar",previous:{title:"Build Multiple Interconnected GCP GKE Clusters",permalink:"/build-multi-gcp-gke"},next:{title:"Deploy a Heterogeneous Cluster for an Existing TiDB Cluster",permalink:"/deploy-heterogeneous-tidb-cluster"}},p={},u=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Supported scenarios",id:"supported-scenarios",level:2},{value:"Deploy a cluster across multiple Kubernetes clusters",id:"deploy-a-cluster-across-multiple-kubernetes-clusters",level:2},{value:"Deploy the initial cluster",id:"deploy-the-initial-cluster",level:3},{value:"Deploy the new cluster to join the initial cluster",id:"deploy-the-new-cluster-to-join-the-initial-cluster",level:3},{value:"Deploy the TLS-enabled TiDB cluster across multiple Kubernetes clusters",id:"deploy-the-tls-enabled-tidb-cluster-across-multiple-kubernetes-clusters",level:2},{value:"Issue the root certificate",id:"issue-the-root-certificate",level:3},{value:"Use <code>cfssl</code>",id:"use-cfssl",level:4},{value:"Use <code>cert-manager</code>",id:"use-cert-manager",level:4},{value:"Issue certificates for the TiDB components of each Kubernetes cluster",id:"issue-certificates-for-the-tidb-components-of-each-kubernetes-cluster",level:3},{value:"Use the <code>cfssl</code> system to issue certificates for TiDB components",id:"use-the-cfssl-system-to-issue-certificates-for-tidb-components",level:4},{value:"Use the <code>cert-manager</code> system to issue certificates for TiDB components",id:"use-the-cert-manager-system-to-issue-certificates-for-tidb-components",level:4},{value:"Deploy the initial cluster",id:"deploy-the-initial-cluster-1",level:3},{value:"Deploy a new cluster to join the initial cluster",id:"deploy-a-new-cluster-to-join-the-initial-cluster",level:3},{value:"Upgrade TiDB Cluster",id:"upgrade-tidb-cluster",level:2},{value:"Exit and reclaim clusters that already join a cross-Kubernetes cluster",id:"exit-and-reclaim-clusters-that-already-join-a-cross-kubernetes-cluster",level:2},{value:"Enable the feature for a cluster with existing data and make it the initial TiDB cluster",id:"enable-the-feature-for-a-cluster-with-existing-data-and-make-it-the-initial-tidb-cluster",level:2}],m={toc:u};function d(e){var t=e.components,n=(0,r.Z)(e,l);return(0,s.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"This is still an experimental feature. It is ",(0,s.kt)("strong",{parentName:"p"},"NOT")," recommended that you use it in the production environment."))),(0,s.kt)("h1",{id:"deploy-a-tidb-cluster-across-multiple-kubernetes-clusters"},"Deploy a TiDB Cluster across Multiple Kubernetes Clusters"),(0,s.kt)("p",null,"To deploy a TiDB cluster across multiple Kubernetes clusters refers to deploying ",(0,s.kt)("strong",{parentName:"p"},"one")," TiDB cluster on multiple interconnected Kubernetes clusters. Each component of the cluster is distributed on multiple Kubernetes clusters to achieve disaster recovery among Kubernetes clusters. The interconnected network of Kubernetes clusters means that Pod IP can be accessed in any cluster and between clusters, and Pod FQDN records can be looked up by querying the DNS service in any cluster and between clusters."),(0,s.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,s.kt)("p",null,"You need to configure the Kubernetes network and DNS so that the Kubernetes cluster meets the following conditions:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The TiDB components on each Kubernetes cluster can access the Pod IP of all TiDB components in and between clusters."),(0,s.kt)("li",{parentName:"ul"},"The TiDB components on each Kubernetes cluster can look up the Pod FQDN of all TiDB components in and between clusters.")),(0,s.kt)("p",null,"To build multiple connected EKS or GKE clusters, refer to ",(0,s.kt)("a",{parentName:"p",href:"/build-multi-aws-eks"},"Build Multiple Interconnected AWS EKS Clusters")," or ",(0,s.kt)("a",{parentName:"p",href:"/build-multi-gcp-gke"},"Build Multiple Interconnected GCP GKE Clusters"),"."),(0,s.kt)("h2",{id:"supported-scenarios"},"Supported scenarios"),(0,s.kt)("p",null,"Currently supported scenarios:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Deploy a new TiDB cluster across multiple Kubernetes clusters."),(0,s.kt)("li",{parentName:"ul"},"Deploy new TiDB clusters that enable this feature on other Kubernetes clusters and join the initial TiDB cluster.")),(0,s.kt)("p",null,"Experimentally supported scenarios:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Enable this feature for a cluster that already has data. If you need to perform this action in a production environment, it is recommended to complete this requirement through data migration.")),(0,s.kt)("p",null,"Unsupported scenarios:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"You cannot interconnect two clusters that already have data. You might perform this action through data migration.")),(0,s.kt)("h2",{id:"deploy-a-cluster-across-multiple-kubernetes-clusters"},"Deploy a cluster across multiple Kubernetes clusters"),(0,s.kt)("p",null,"Before you deploy a TiDB cluster across multiple Kubernetes clusters, you need to first deploy the Kubernetes clusters required for this operation. The following deployment assumes that you have completed Kubernetes deployment."),(0,s.kt)("p",null,"The following takes the deployment of two clusters as an example. Cluster #1 is the initial cluster. Create it according to the configuration given below. After cluster #1 is running normally, create cluster #2 according to the configuration given below. After creating and deploying clusters, two clusters run normally."),(0,s.kt)("h3",{id:"deploy-the-initial-cluster"},"Deploy the initial cluster"),(0,s.kt)("p",null,"Set the following environment variables according to the actual situation. You need to set the contents of the ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_name")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_cluster_domain")," variables according to your actual use. ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_name")," is the cluster name of cluster #1, ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_cluster_domain")," is the ",(0,s.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#introduction"},"Cluster Domain")," of cluster #1, and ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_namespace")," is the namespace of cluster #1."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'\ncluster1_name="cluster1"\ncluster1_cluster_domain="cluster1.com"\ncluster1_namespace="pingcap"\n')),(0,s.kt)("p",null,"Run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat << EOF | kubectl apply -n ${cluster1_namespace} -f -\napiVersion: pingcap.com/v1alpha1\nkind: TidbCluster\nmetadata:\n  name: "${cluster1_name}"\nspec:\n  version: v4.0.9\n  timezone: UTC\n  pvReclaimPolicy: Delete\n  enableDynamicConfiguration: true\n  configUpdateStrategy: RollingUpdate\n  clusterDomain: "${cluster1_cluster_domain}"\n  discovery: {}\n  pd:\n    baseImage: pingcap/pd\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config: {}\n  tikv:\n    baseImage: pingcap/tikv\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config: {}\n  tidb:\n    baseImage: pingcap/tidb\n    maxFailoverCount: 0\n    replicas: 1\n    service:\n      type: ClusterIP\n    config: {}\nEOF\n')),(0,s.kt)("h3",{id:"deploy-the-new-cluster-to-join-the-initial-cluster"},"Deploy the new cluster to join the initial cluster"),(0,s.kt)("p",null,"You can wait for the cluster #1 to complete the deployment, and then create cluster #2. In the actual situation, cluster #2 refers to the cluster you newly created. You can create a new cluster to join any existing cluster in multiple clusters."),(0,s.kt)("p",null,"Refer to the following example and fill in the relevant information such as ",(0,s.kt)("inlineCode",{parentName:"p"},"Name"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"Cluster Domain"),", and ",(0,s.kt)("inlineCode",{parentName:"p"},"Namespace")," of cluster #1 and cluster #2 according to the actual situation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster1_name="cluster1"\ncluster1_cluster_domain="cluster1.com"\ncluster1_namespace="pingcap"\ncluster2_name="cluster2"\ncluster2_cluster_domain="cluster2.com"\ncluster2_namespace="pingcap"\n')),(0,s.kt)("p",null,"Run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat << EOF | kubectl apply -n ${cluster2_namespace} -f -\napiVersion: pingcap.com/v1alpha1\nkind: TidbCluster\nmetadata:\n  name: "${cluster2_name}"\nspec:\n  version: v4.0.9\n  timezone: UTC\n  pvReclaimPolicy: Delete\n  enableDynamicConfiguration: true\n  configUpdateStrategy: RollingUpdate\n  clusterDomain: "${cluster2_cluster_domain}"\n  cluster:\n    name: "${cluster1_name}"\n    namespace: "${cluster1_namespace}"\n    clusterDomain: "${cluster1_clusterdomain}"\n  discovery: {}\n  pd:\n    baseImage: pingcap/pd\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config: {}\n  tikv:\n    baseImage: pingcap/tikv\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config: {}\n  tidb:\n    baseImage: pingcap/tidb\n    maxFailoverCount: 0\n    replicas: 1\n    service:\n      type: ClusterIP\n    config: {}\nEOF\n')),(0,s.kt)("h2",{id:"deploy-the-tls-enabled-tidb-cluster-across-multiple-kubernetes-clusters"},"Deploy the TLS-enabled TiDB cluster across multiple Kubernetes clusters"),(0,s.kt)("p",null,"You can follow the steps below to enable TLS between TiDB components for TiDB clusters deployed across multiple Kubernetes clusters."),(0,s.kt)("h3",{id:"issue-the-root-certificate"},"Issue the root certificate"),(0,s.kt)("h4",{id:"use-cfssl"},"Use ",(0,s.kt)("inlineCode",{parentName:"h4"},"cfssl")),(0,s.kt)("p",null,"If you use ",(0,s.kt)("inlineCode",{parentName:"p"},"cfssl"),", the CA certificate issue process is the same as the general issue process. You need to save the CA certificate created for the first time, and use this CA certificate when you issue certificates for TiDB components later."),(0,s.kt)("p",null,"In other words, when you create a component certificate in a cluster, you do not need to create a CA certificate again. Complete step 1 ~ 4 in ",(0,s.kt)("a",{parentName:"p",href:"/enable-tls-between-components#using-cfssl"},"Enabling TLS between TiDB components")," once to issue the CA certificate. After that, start from step 5 to issue certificates between other cluster components."),(0,s.kt)("h4",{id:"use-cert-manager"},"Use ",(0,s.kt)("inlineCode",{parentName:"h4"},"cert-manager")),(0,s.kt)("p",null,"If you use ",(0,s.kt)("inlineCode",{parentName:"p"},"cert-manager"),", you only need to create a ",(0,s.kt)("inlineCode",{parentName:"p"},"CA Issuer")," and a ",(0,s.kt)("inlineCode",{parentName:"p"},"CA Certificate")," in the initial cluster, and export the ",(0,s.kt)("inlineCode",{parentName:"p"},"CA Secret")," to other new clusters that want to join."),(0,s.kt)("p",null,"For other clusters, you only need to create a component certificate ",(0,s.kt)("inlineCode",{parentName:"p"},"Issuer")," (refers to ",(0,s.kt)("inlineCode",{parentName:"p"},"${cluster_name}-tidb-issuer")," in the ",(0,s.kt)("a",{parentName:"p",href:"/enable-tls-between-components#using-cert-manager"},"TLS document"),") and configure the ",(0,s.kt)("inlineCode",{parentName:"p"},"Issuer")," to use the ",(0,s.kt)("inlineCode",{parentName:"p"},"CA"),". The detailed process is as follows:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Create a ",(0,s.kt)("inlineCode",{parentName:"p"},"CA Issuer")," and a ",(0,s.kt)("inlineCode",{parentName:"p"},"CA Certificate")," in the initial cluster."),(0,s.kt)("p",{parentName:"li"},"Set the following environment variables according to the actual situation:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster_name="cluster1"\nnamespace="pingcap"\n')),(0,s.kt)("p",{parentName:"li"},"Run the following command:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat <<EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ${cluster_name}-selfsigned-ca-issuer\n  namespace: ${namespace}\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: ${cluster_name}-ca\n  namespace: ${namespace}\nspec:\n  secretName: ${cluster_name}-ca-secret\n  commonName: "TiDB"\n  isCA: true\n  duration: 87600h # 10yrs\n  renewBefore: 720h # 30d\n  issuerRef:\n    name: ${cluster_name}-selfsigned-ca-issuer\n    kind: Issuer\nEOF\n'))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Export the CA and delete irrelevant information."),(0,s.kt)("p",{parentName:"li"},"First, you need to export the ",(0,s.kt)("inlineCode",{parentName:"p"},"Secret")," that stores the CA. The name of the ",(0,s.kt)("inlineCode",{parentName:"p"},"Secret")," can be obtained from ",(0,s.kt)("inlineCode",{parentName:"p"},".spec.secretName")," of the ",(0,s.kt)("inlineCode",{parentName:"p"},"Certificate")," YAML file in the first step."),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"}," kubectl get secret cluster1-ca-secret -n ${namespace} -o yaml > ca.yaml\n")),(0,s.kt)("p",{parentName:"li"},"Delete irrelevant information in the Secret YAML file. After the deletion, the YAML file is as follows (the information in ",(0,s.kt)("inlineCode",{parentName:"p"},"data")," is omitted):"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\ndata:\n  ca.crt: LS0...LQo=\n  tls.crt: LS0t....LQo=\n  tls.key: LS0t...tCg==\nkind: Secret\nmetadata:\n  name: cluster1-ca-secret\ntype: kubernetes.io/tls\n"))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Import the exported CA to other clusters."),(0,s.kt)("p",{parentName:"li"},"You need to configure the ",(0,s.kt)("inlineCode",{parentName:"p"},"namespace")," so that related components can access the CA certificate:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f ca.yaml -n ${namespace}\n"))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Create a component certificate ",(0,s.kt)("inlineCode",{parentName:"p"},"Issuer")," in the initial cluster and the new cluster, and configure it to use this CA."),(0,s.kt)("ol",{parentName:"li"},(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Create an ",(0,s.kt)("inlineCode",{parentName:"p"},"Issuer")," that issues certificates between TiDB components in the initial cluster."),(0,s.kt)("p",{parentName:"li"},"Set the following environment variables according to the actual situation:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster_name="cluster1"\nnamespace="pingcap"\nca_secret_name="cluster1-ca-secret"\n')),(0,s.kt)("p",{parentName:"li"},"Run the following command:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"cat << EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ${cluster_name}-tidb-issuer\n  namespace: ${namespace}\nspec:\n  ca:\n    secretName: ${ca_secret_name}\nEOF\n"))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Create an ",(0,s.kt)("inlineCode",{parentName:"p"},"Issuer")," that issues certificates between TiDB components in the new cluster."),(0,s.kt)("p",{parentName:"li"}," Set the following environment variables according to the actual situation. Among them, ",(0,s.kt)("inlineCode",{parentName:"p"},"ca_secret_name")," points to the imported ",(0,s.kt)("inlineCode",{parentName:"p"},"Secret")," that stores the ",(0,s.kt)("inlineCode",{parentName:"p"},"CA"),". You can use the ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster_name")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"namespace")," in the following operations:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster_name="cluster2"\nnamespace="pingcap"\nca_secret_name="cluster1-ca-secret"\n')),(0,s.kt)("p",{parentName:"li"},"Run the following command:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"cat << EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ${cluster_name}-tidb-issuer\n  namespace: ${namespace}\nspec:\n  ca:\n    secretName: ${ca_secret_name}\nEOF\n")))))),(0,s.kt)("h3",{id:"issue-certificates-for-the-tidb-components-of-each-kubernetes-cluster"},"Issue certificates for the TiDB components of each Kubernetes cluster"),(0,s.kt)("p",null,"You need to issue a component certificate for each TiDB component on the Kubernetes cluster. When issuing a component certificate, you need to add an authorization record ending with ",(0,s.kt)("inlineCode",{parentName:"p"},".${cluster_domain}")," to the hosts, for example, ",(0,s.kt)("inlineCode",{parentName:"p"},"${cluster_name}-pd.${namespace}.svc.${cluster_domain}"),"."),(0,s.kt)("h4",{id:"use-the-cfssl-system-to-issue-certificates-for-tidb-components"},"Use the ",(0,s.kt)("inlineCode",{parentName:"h4"},"cfssl")," system to issue certificates for TiDB components"),(0,s.kt)("p",null,"The following example shows how to use ",(0,s.kt)("inlineCode",{parentName:"p"},"cfssl")," to create a certificate used by PD. The ",(0,s.kt)("inlineCode",{parentName:"p"},"pd-server.json")," file is as follows."),(0,s.kt)("p",null,"Set the following environment variables according to the actual situation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"cluster_name=cluster2\ncluster_domain=cluster2.com\nnamespace=pingcap\n")),(0,s.kt)("p",null,"You can create the ",(0,s.kt)("inlineCode",{parentName:"p"},"pd-server.json")," by the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat << EOF > pd-server.json\n{\n    "CN": "TiDB",\n    "hosts": [\n      "127.0.0.1",\n      "::1",\n      "${cluster_name}-pd",\n      "${cluster_name}-pd.${namespace}",\n      "${cluster_name}-pd.${namespace}.svc",\n      "${cluster_name}-pd.${namespace}.svc.${cluster_domain}",\n      "${cluster_name}-pd-peer",\n      "${cluster_name}-pd-peer.${namespace}",\n      "${cluster_name}-pd-peer.${namespace}.svc",\n      "${cluster_name}-pd-peer.${namespace}.svc.${cluster_domain}",\n      "*.${cluster_name}-pd-peer",\n      "*.${cluster_name}-pd-peer.${namespace}",\n      "*.${cluster_name}-pd-peer.${namespace}.svc",\n      "*.${cluster_name}-pd-peer.${namespace}.svc.${cluster_domain}"\n    ],\n    "key": {\n        "algo": "ecdsa",\n        "size": 256\n    },\n    "names": [\n        {\n            "C": "US",\n            "L": "CA",\n            "ST": "San Francisco"\n        }\n    ]\n}\nEOF\n')),(0,s.kt)("h4",{id:"use-the-cert-manager-system-to-issue-certificates-for-tidb-components"},"Use the ",(0,s.kt)("inlineCode",{parentName:"h4"},"cert-manager")," system to issue certificates for TiDB components"),(0,s.kt)("p",null,"The following example shows how to use ",(0,s.kt)("inlineCode",{parentName:"p"},"cert-manager")," to create a certificate used by PD. ",(0,s.kt)("inlineCode",{parentName:"p"},"Certifcates")," is shown below."),(0,s.kt)("p",null,"Set the following environment variables according to the actual situation."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster_name="cluster2"\nnamespace="pingcap"\ncluster_domain="cluster2.com"\n')),(0,s.kt)("p",null,"Run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat << EOF | kubectl apply -f -\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: ${cluster_name}-pd-cluster-secret\n  namespace: ${namespace}\nspec:\n  secretName: ${cluster_name}-pd-cluster-secret\n  duration: 8760h # 365d\n  renewBefore: 360h # 15d\n  subject:\n    organizations:\n    - PingCAP\n  commonName: "TiDB"\n  usages:\n    - server auth\n    - client auth\n  dnsNames:\n    - "${cluster_name}-pd"\n    - "${cluster_name}-pd.${namespace}"\n    - "${cluster_name}-pd.${namespace}.svc"\n    - "${cluster_name}-pd.${namespace}.svc.${cluster_domain}"\n    - "${cluster_name}-pd-peer"\n    - "${cluster_name}-pd-peer.${namespace}"\n    - "${cluster_name}-pd-peer.${namespace}.svc"\n    - "${cluster_name}-pd-peer.${namespace}.svc.${cluster_domain}"\n    - "*.${cluster_name}-pd-peer"\n    - "*.${cluster_name}-pd-peer.${namespace}"\n    - "*.${cluster_name}-pd-peer.${namespace}.svc"\n    - "*.${cluster_name}-pd-peer.${namespace}.svc.${cluster_domain}"\n  ipAddresses:\n  - 127.0.0.1\n  - ::1\n  issuerRef:\n    name: ${cluster_name}-tidb-issuer\n    kind: Issuer\n    group: cert-manager.io\nEOF\n')),(0,s.kt)("p",null,"You need to refer to the TLS-related documents, issue the corresponding certificates for the components, and create the ",(0,s.kt)("inlineCode",{parentName:"p"},"Secret")," in the corresponding Kubernetes clusters."),(0,s.kt)("p",null,"For other TLS-related information, refer to the following documents:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/enable-tls-between-components"},"Enable TLS between TiDB Components")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/enable-tls-for-mysql-client"},"Enable TLS for the MySQL Client"))),(0,s.kt)("h3",{id:"deploy-the-initial-cluster-1"},"Deploy the initial cluster"),(0,s.kt)("p",null,"This section introduces how to deploy and initialize the cluster."),(0,s.kt)("p",null,"In actual use, you need to set the contents of the ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_name")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_cluster_domain")," variables according to your actual situation, where ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_name")," is the cluster name of cluster #1, ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_cluster_domain")," is the ",(0,s.kt)("inlineCode",{parentName:"p"},"Cluster Domain")," of cluster #1, and ",(0,s.kt)("inlineCode",{parentName:"p"},"cluster1_namespace")," is the namespace of cluster #1. The following ",(0,s.kt)("inlineCode",{parentName:"p"},"YAML")," file enables the TLS feature, and each component starts to verify the certificates issued by the ",(0,s.kt)("inlineCode",{parentName:"p"},"CN")," for the ",(0,s.kt)("inlineCode",{parentName:"p"},"CA")," of ",(0,s.kt)("inlineCode",{parentName:"p"},"TiDB")," by configuring the ",(0,s.kt)("inlineCode",{parentName:"p"},"cert-allowed-cn"),"."),(0,s.kt)("p",null,"Set the following environment variables according to the actual situation."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster1_name="cluster1"\ncluster1_cluster_domain="cluster1.com"\ncluster1_namespace="pingcap"\n')),(0,s.kt)("p",null,"Run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'cat << EOF | kubectl apply -n ${cluster1_namespace} -f -\napiVersion: pingcap.com/v1alpha1\nkind: TidbCluster\nmetadata:\n  name: "${cluster1_name}"\nspec:\n  version: v4.0.9\n  timezone: UTC\n  tlsCluster:\n   enabled: true\n  pvReclaimPolicy: Delete\n  enableDynamicConfiguration: true\n  configUpdateStrategy: RollingUpdate\n  clusterDomain: "${cluster1_cluster_domain}"\n  discovery: {}\n  pd:\n    baseImage: pingcap/pd\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config:\n      security:\n        cert-allowed-cn:\n          - TiDB\n  tikv:\n    baseImage: pingcap/tikv\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config:\n      security:\n       cert-allowed-cn:\n         - TiDB\n  tidb:\n    baseImage: pingcap/tidb\n    maxFailoverCount: 0\n    replicas: 1\n    service:\n      type: ClusterIP\n    tlsClient:\n      enabled: true\n    config:\n      security:\n       cert-allowed-cn:\n         - TiDB\nEOF\n')),(0,s.kt)("h3",{id:"deploy-a-new-cluster-to-join-the-initial-cluster"},"Deploy a new cluster to join the initial cluster"),(0,s.kt)("p",null,"You can wait for the cluster #1 to complete the deployment. After completing the deployment, you can create cluster #2. The related commands are as follows. In actual use, cluster #1 might not the initial cluster. You can specify cluster #2 to join any cluster in the multiple clusters."),(0,s.kt)("p",null,"Set the following environment variables according to the actual situation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cluster1_name="cluster1"\ncluster1_cluster_domain="cluster1.com"\ncluster1_namespace="pingcap"\ncluster2_name="cluster2"\ncluster2_cluster_domain="cluster2.com"\ncluster2_namespace="pingcap"\n')),(0,s.kt)("p",null,"Run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'cat << EOF | kubectl apply -n ${cluster2_namespace} -f -\napiVersion: pingcap.com/v1alpha1\nkind: TidbCluster\nmetadata:\n  name: "${cluster2_name}"\nspec:\n  version: v4.0.9\n  timezone: UTC\n  tlsCluster:\n   enabled: true\n  pvReclaimPolicy: Delete\n  enableDynamicConfiguration: true\n  configUpdateStrategy: RollingUpdate\n  clusterDomain: "${cluster2_cluster_domain}"\n  cluster:\n    name: "${cluster1_name}"\n    namespace: "${cluster1_namespace}"\n    clusterDomain: "${cluster1_clusterdomain}"\n  discovery: {}\n  pd:\n    baseImage: pingcap/pd\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config:\n      security:\n        cert-allowed-cn:\n          - TiDB\n  tikv:\n    baseImage: pingcap/tikv\n    maxFailoverCount: 0\n    replicas: 1\n    requests:\n      storage: "10Gi"\n    config:\n      security:\n       cert-allowed-cn:\n         - TiDB\n  tidb:\n    baseImage: pingcap/tidb\n    maxFailoverCount: 0\n    replicas: 1\n    service:\n      type: ClusterIP\n    tlsClient:\n      enabled: true\n    config:\n      security:\n       cert-allowed-cn:\n         - TiDB\nEOF\n')),(0,s.kt)("h2",{id:"upgrade-tidb-cluster"},"Upgrade TiDB Cluster"),(0,s.kt)("p",null,"For a TiDB cluster deployed across Kubernetes clusters, to perform a rolling upgrade for each component Pod of the TiDB cluster, take the following steps in sequence to modify the version configuration of each component in the TidbCluster spec for each Kubernetes cluster."),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Upgrade PD versions for all Kubernetes clusters."),(0,s.kt)("ol",{parentName:"li"},(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Modify the ",(0,s.kt)("inlineCode",{parentName:"p"},"spec.pd.version")," field in the spec for cluster #1."),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: pingcap.com/v1alpha1\nkind: TidbCluster\n# ...\nspec:\n  pd:\n    version: ${version}\n"))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Watch the status of PD Pods and wait for PD Pods in cluster #1 to finish recreation and become ",(0,s.kt)("inlineCode",{parentName:"p"},"Running"),".")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Repeat the first two substeps to upgrade all PD Pods in other clusters.")))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Take step 1 as an example, perform the following upgrade operations in sequence:"),(0,s.kt)("ol",{parentName:"li"},(0,s.kt)("li",{parentName:"ol"},"If TiFlash is deployed in clusters, upgrade the TiFlash versions for all the Kubernetes clusters that have TiFlash deployed."),(0,s.kt)("li",{parentName:"ol"},"Upgrade TiKV versions for all Kubernetes clusters."),(0,s.kt)("li",{parentName:"ol"},"If Pump is deployed in clusters, upgrade the Pump versions for all the Kubernetes clusters that have Pump deployed."),(0,s.kt)("li",{parentName:"ol"},"Upgrade TiDM versions for all Kubernetes clusters."),(0,s.kt)("li",{parentName:"ol"},"If TiCDC is deployed in clusters, upgrade the TiCDC versions for all the Kubernetes clusters that have TiCDC deployed.")))),(0,s.kt)("h2",{id:"exit-and-reclaim-clusters-that-already-join-a-cross-kubernetes-cluster"},"Exit and reclaim clusters that already join a cross-Kubernetes cluster"),(0,s.kt)("p",null,"When you need to make a cluster exit from the joined TiDB cluster deployed across Kubernetes and reclaim resources, you can perform the operation by scaling in the cluster. In this scenario, the following requirements of scaling-in need to be met."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"After scaling in the cluster, the number of TiKV replicas in the cluster should be greater than the number of ",(0,s.kt)("inlineCode",{parentName:"li"},"max-replicas")," set in PD. By default, the number of TiKV replicas needs to be greater than three.")),(0,s.kt)("p",null,"Take the cluster #2 created in ",(0,s.kt)("a",{parentName:"p",href:"#deploy-a-new-cluster-to-join-the-initial-cluster"},"the last section")," as an example. First, set the number of replicas of PD, TiKV, and TiDB to ",(0,s.kt)("inlineCode",{parentName:"p"},"0"),". If you enable other components such as TiFlash, TiCDC, and Pump, set the number of these replicas to ",(0,s.kt)("inlineCode",{parentName:"p"},"0"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl patch tc cluster2 --type merge -p \'{"spec":{"pd":{"replicas":0},"tikv":{"replicas":0},"tidb":{"replicas":0}}}\'\n')),(0,s.kt)("p",null,"Wait for the status of cluster #2 to become ",(0,s.kt)("inlineCode",{parentName:"p"},"Ready"),", and scale in related components to ",(0,s.kt)("inlineCode",{parentName:"p"},"0")," replica:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods -l app.kubernetes.io/instance=cluster2 -n pingcap\n")),(0,s.kt)("p",null,"The Pod list shows ",(0,s.kt)("inlineCode",{parentName:"p"},"No resources found"),". At this time, Pods have all been scaled in, and cluster #2 exits the cluster. Check the cluster status of cluster #2:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get tc cluster2\n")),(0,s.kt)("p",null,"The result shows that cluster #2 is in the ",(0,s.kt)("inlineCode",{parentName:"p"},"Ready")," status. At this time, you can delete the object and reclaim related resources."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete tc cluster2\n")),(0,s.kt)("p",null,"Through the above steps, you can complete exit and resources reclaim of the joined clusters."),(0,s.kt)("h2",{id:"enable-the-feature-for-a-cluster-with-existing-data-and-make-it-the-initial-tidb-cluster"},"Enable the feature for a cluster with existing data and make it the initial TiDB cluster"),(0,s.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,s.kt)("div",{parentName:"div",className:"admonition-heading"},(0,s.kt)("h5",{parentName:"div"},(0,s.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,s.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,s.kt)("div",{parentName:"div",className:"admonition-content"},(0,s.kt)("p",{parentName:"div"},"Currently, this is an experimental feature and might cause data loss. Please use it carefully."))),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Update ",(0,s.kt)("inlineCode",{parentName:"p"},".spec.clusterDomain")," configuration:"),(0,s.kt)("p",{parentName:"li"},"Configure the following parameters according to the ",(0,s.kt)("inlineCode",{parentName:"p"},"clusterDomain")," in your Kubernetes cluster information:"),(0,s.kt)("blockquote",{parentName:"li"},(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("strong",{parentName:"p"},"Warning:")),(0,s.kt)("p",{parentName:"blockquote"},"Currently, you need to configure ",(0,s.kt)("inlineCode",{parentName:"p"},"clusterDomain")," with correct information. After modifying the configuration, you can not modify it again.")),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl patch tidbcluster cluster1 --type merge -p \'{"spec":{"clusterDomain":"cluster1.com"}}\'\n')),(0,s.kt)("p",{parentName:"li"},"After completing the modification, the TiDB cluster performs the rolling update.")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Update the ",(0,s.kt)("inlineCode",{parentName:"p"},"PeerURL")," information of PD:"),(0,s.kt)("p",{parentName:"li"},"After completing the rolling update, you need to use ",(0,s.kt)("inlineCode",{parentName:"p"},"port-forward")," to expose PD's API, and use API of PD to update ",(0,s.kt)("inlineCode",{parentName:"p"},"PeerURL")," of PD."),(0,s.kt)("ol",{parentName:"li"},(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Use ",(0,s.kt)("inlineCode",{parentName:"p"},"port-forward")," to expose API of PD:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward pods/cluster1-pd-0 2380:2380 2379:2379 -n pingcap\n"))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Access ",(0,s.kt)("inlineCode",{parentName:"p"},"PD API")," to obtain ",(0,s.kt)("inlineCode",{parentName:"p"},"members")," information. Note that after using ",(0,s.kt)("inlineCode",{parentName:"p"},"port-forward"),", the terminal session is occupied. You need to perform the following operations in another terminal session:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"curl http://127.0.0.1:2379/v2/members\n")),(0,s.kt)("blockquote",{parentName:"li"},(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("strong",{parentName:"p"},"Note:")),(0,s.kt)("p",{parentName:"blockquote"},"If the cluster enables TLS, you need to configure the certificate when using the curl command. For example:"),(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("inlineCode",{parentName:"p"},"curl --cacert /var/lib/pd-tls/ca.crt --cert /var/lib/pd-tls/tls.crt --key /var/lib/pd-tls/tls.key https://127.0.0.1:2379/v2/members"))),(0,s.kt)("p",{parentName:"li"},"After running the command, the output is as follows:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-output"},'{"members":[{"id":"6ed0312dc663b885","name":"cluster1-pd-0.cluster1-pd-peer.pingcap.svc.cluster1.com","peerURLs":["http://cluster1-pd-0.cluster1-pd-peer.pingcap.svc:2380"],"clientURLs":["http://cluster1-pd-0.cluster1-pd-peer.pingcap.svc.cluster1.com:2379"]},{"id":"bd9acd3d57e24a32","name":"cluster1-pd-1.cluster1-pd-peer.pingcap.svc.cluster1.com","peerURLs":["http://cluster1-pd-1.cluster1-pd-peer.pingcap.svc:2380"],"clientURLs":["http://cluster1-pd-1.cluster1-pd-peer.pingcap.svc.cluster1.com:2379"]},{"id":"e04e42cccef60246","name":"cluster1-pd-2.cluster1-pd-peer.pingcap.svc.cluster1.com","peerURLs":["http://cluster1-pd-2.cluster1-pd-peer.pingcap.svc:2380"],"clientURLs":["http://cluster1-pd-2.cluster1-pd-peer.pingcap.svc.cluster1.com:2379"]}]}\n'))),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("p",{parentName:"li"},"Record the ",(0,s.kt)("inlineCode",{parentName:"p"},"id")," of each PD instance, and use the ",(0,s.kt)("inlineCode",{parentName:"p"},"id")," to update the ",(0,s.kt)("inlineCode",{parentName:"p"},"peerURL")," of each member in turn:"),(0,s.kt)("pre",{parentName:"li"},(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'member_ID="6ed0312dc663b885"\nmember_peer_url="http://cluster1-pd-0.cluster1-pd-peer.pingcap.svc.cluster1.com:2380"\ncurl http://127.0.0.1:2379/v2/members/${member_ID} -XPUT \\\n-H "Content-Type: application/json" -d \'{"peerURLs":["${member_peer_url}"]}\'\n')))))),(0,s.kt)("p",null,"For more examples and development information, refer to ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/pingcap/tidb-operator/tree/master/examples/multi-cluster"},(0,s.kt)("inlineCode",{parentName:"a"},"multi-cluster")),"."))}d.isMDXComponent=!0}}]);