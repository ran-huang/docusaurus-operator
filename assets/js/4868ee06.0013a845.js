"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[3552],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return u}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(a),u=o,k=d["".concat(l,".").concat(u)]||d[u]||m[u]||r;return a?n.createElement(k,i(i({ref:t},c),{},{components:a})):n.createElement(k,i({ref:t},c))}));function u(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6783:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},assets:function(){return c},toc:function(){return m},default:function(){return u}});var n=a(7462),o=a(3366),r=(a(7294),a(3905)),i=["components"],s={title:"Persistent Storage Class Configuration in Kubernetes",summary:"Learn how to configure local PVs and network PVs."},l="Persistent Storage Class Configuration in Kubernetes",p={unversionedId:"configure-storage-class",id:"configure-storage-class",title:"Persistent Storage Class Configuration in Kubernetes",description:"TiDB cluster components such as PD, TiKV, TiDB monitoring, TiDB Binlog, and tidb-backup require the persistent storage of data. To persist the data in Kubernetes, you need to use PersistentVolume (PV). Kubernetes supports several types of storage classes, which are mainly divided into two parts:",source:"@site/docs/configure-storage-class.md",sourceDirName:".",slug:"/configure-storage-class",permalink:"/configure-storage-class",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/configure-storage-class.md",tags:[],version:"current",frontMatter:{title:"Persistent Storage Class Configuration in Kubernetes",summary:"Learn how to configure local PVs and network PVs."},sidebar:"mySidebar",previous:{title:"Prerequisites for TiDB in Kubernetes",permalink:"/prerequisites"},next:{title:"Deploy TiDB Operator in Kubernetes",permalink:"/deploy-tidb-operator"}},c={},m=[{value:"Recommended storage classes for TiDB clusters",id:"recommended-storage-classes-for-tidb-clusters",level:2},{value:"Network PV configuration",id:"network-pv-configuration",level:2},{value:"Local PV configuration",id:"local-pv-configuration",level:2},{value:"Best practices",id:"best-practices",level:3},{value:"Disk mount examples",id:"disk-mount-examples",level:2},{value:"Data safety",id:"data-safety",level:2},{value:"Delete PV and data",id:"delete-pv-and-data",level:3}],d={toc:m};function u(e){var t=e.components,a=(0,o.Z)(e,i);return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"persistent-storage-class-configuration-in-kubernetes"},"Persistent Storage Class Configuration in Kubernetes"),(0,r.kt)("p",null,"TiDB cluster components such as PD, TiKV, TiDB monitoring, TiDB Binlog, and ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-backup")," require the persistent storage of data. To persist the data in Kubernetes, you need to use ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/"},"PersistentVolume (PV)"),". Kubernetes supports several types of ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/volumes/"},"storage classes"),", which are mainly divided into two parts:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Network storage"),(0,r.kt)("p",{parentName:"li"},"  The network storage medium is not on the current node but is mounted to the node through the network. Generally, there are redundant replicas to guarantee high availability. When the node fails, the corresponding network storage can be re-mounted to another node for further use.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Local storage"),(0,r.kt)("p",{parentName:"li"},"  The local storage medium is on the current node and typically can provide lower latency than the network storage. Because there are no redundant replicas, once the node fails, data might be lost. If it is an IDC server, data can be restored to a certain extent. If it is a virtual machine using the local disk on the public cloud, data ",(0,r.kt)("strong",{parentName:"p"},"cannot")," be retrieved after the node fails."))),(0,r.kt)("p",null,"PVs are created automatically by the system administrator or volume provisioner. PVs and Pods are bound by ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims"},"PersistentVolumeClaim (PVC)"),". Users request for using a PV through a PVC instead of creating a PV directly. The corresponding volume provisioner creates a PV that meets the requirements of PVC and then binds the PV to the PVC."),(0,r.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Do not delete a PV in any case unless you are familiar with the underlying volume provisioner. Deleting a PV manually can cause orphaned volumes and unexpected behavior."))),(0,r.kt)("h2",{id:"recommended-storage-classes-for-tidb-clusters"},"Recommended storage classes for TiDB clusters"),(0,r.kt)("p",null,"TiKV uses the Raft protocol to replicate data. When a node fails, PD automatically schedules data to fill the missing data replicas; TiKV requires low read and write latency, so local SSD storage is strongly recommended in the production environment."),(0,r.kt)("p",null,"PD also uses Raft to replicate data. PD is not an I/O-intensive application, but a database for storing cluster meta information, so a local SAS disk or network SSD storage such as EBS General Purpose SSD (gp2) volumes on AWS or SSD persistent disks on GCP can meet the requirements."),(0,r.kt)("p",null,"To ensure availability, it is recommended to use network storage for components such as TiDB monitoring, TiDB Binlog, and ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-backup")," because they do not have redundant replicas. TiDB Binlog's Pump and Drainer components are I/O-intensive applications that require low read and write latency, so it is recommended to use high-performance network storage such as EBS Provisioned IOPS SSD (io1) volumes on AWS or SSD persistent disks on GCP."),(0,r.kt)("p",null,"When deploying TiDB clusters or ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-backup")," with TiDB Operator, you can configure ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for the components that require persistent storage via the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"storageClassName")," field in the ",(0,r.kt)("inlineCode",{parentName:"p"},"values.yaml")," configuration file. The ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClassName")," is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"local-storage")," by default."),(0,r.kt)("h2",{id:"network-pv-configuration"},"Network PV configuration"),(0,r.kt)("p",null,"Kubernetes 1.11 and later versions support ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/"},"volume expansion of network PV"),", but you need to run the following command to enable volume expansion for the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl patch storageclass ${storage_class} -p '{\"allowVolumeExpansion\": true}'\n")),(0,r.kt)("p",null,"After volume expansion is enabled, expand the PV using the following method:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Edit the PersistentVolumeClaim (PVC) object:"),(0,r.kt)("p",{parentName:"li"},"Suppose the PVC is 10 Gi and now we need to expand it to 100 Gi."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl patch pvc -n ${namespace} ${pvc_name} -p \'{"spec": {"resources": {"requests": {"storage": "100Gi"}}}\'\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"View the size of the PV:"),(0,r.kt)("p",{parentName:"li"},"After the expansion, the size displayed by running ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl get pvc -n ${namespace} ${pvc_name}")," is still the original one. But if you run the following command to view the size of the PV, it shows that the size has been expanded to the expected one."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pv | grep ${pvc_name}\n")))),(0,r.kt)("h2",{id:"local-pv-configuration"},"Local PV configuration"),(0,r.kt)("p",null,"Kubernetes currently supports statically allocated local storage. To create a local storage object, use ",(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner")," in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner"},"local-static-provisioner")," repository."),(0,r.kt)("p",null,"The following process uses ",(0,r.kt)("inlineCode",{parentName:"p"},"/mnt/disks")," as the discovery directory and ",(0,r.kt)("inlineCode",{parentName:"p"},"local-storage")," as the StorageClass name to create a PV. If you need to use a different data disk or StorageClass for monitoring, backup, or other purposes, refer to the following ",(0,r.kt)("a",{parentName:"p",href:"#disk-mount-examples"},"example")," for configuration."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Pre-allocate local storage in cluster nodes. See the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md"},"operation guide")," provided by Kubernetes.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Deploy ",(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner"),"."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/local-dind/local-volume-provisioner.yaml\n")),(0,r.kt)("p",{parentName:"li"},"If the server has no access to the Internet, download the ",(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner.yaml")," file on a machine with Internet access and then install it."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"wget https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/local-dind/local-volume-provisioner.yaml &&\nkubectl apply -f ./local-volume-provisioner.yaml\n")),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner")," is a DaemonSet that starts a Pod on every Kubernetes worker node. The Pod uses the ",(0,r.kt)("inlineCode",{parentName:"p"},"quay.io/external_storage/local-volume-provisioner:v2.3.4")," image. If the server does not have access to the Internet, download this Docker image on a machine with Internet access:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"docker pull quay.io/external_storage/local-volume-provisioner:v2.3.4\ndocker save -o local-volume-provisioner-v2.3.4.tar quay.io/external_storage/local-volume-provisioner:v2.3.4\n")),(0,r.kt)("p",{parentName:"li"},"Copy the ",(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner-v2.3.4.tar")," file to the server, and execute the ",(0,r.kt)("inlineCode",{parentName:"p"},"docker load")," command to load it on the server:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"docker load -i local-volume-provisioner-v2.3.4.tar\n")),(0,r.kt)("p",{parentName:"li"},"Check the Pod and PV status with the following commands:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get po -n kube-system -l app=local-volume-provisioner &&\nkubectl get pv | grep local-storage\n")),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner")," creates a PV for each mounting point under the discovery directory."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("ul",{parentName:"blockquote"},(0,r.kt)("li",{parentName:"ul"},"On GKE, ",(0,r.kt)("inlineCode",{parentName:"li"},"local-volume-provisioner")," creates a local volume of only 375 GiB in size by default."),(0,r.kt)("li",{parentName:"ul"},"If no mount point is in the discovery directory, no PV is created and the output of ",(0,r.kt)("inlineCode",{parentName:"li"},"kubectl get pv | grep local-storage")," is empty."),(0,r.kt)("li",{parentName:"ul"},"If the StorageClass name is not ",(0,r.kt)("inlineCode",{parentName:"li"},"local-storage"),", you need to replace ",(0,r.kt)("inlineCode",{parentName:"li"},"local-storage")," in ",(0,r.kt)("inlineCode",{parentName:"li"},"kubectl get pv | grep local-storage")," with the actual StorageClass name to confirm the PV status."))))),(0,r.kt)("p",null,"For more information, refer to ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/volumes/#local"},"Kubernetes local storage")," and ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner#overview"},"local-static-provisioner document"),"."),(0,r.kt)("h3",{id:"best-practices"},"Best practices"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"A local PV's path is its unique identifier. To avoid conflicts, it is recommended to use the UUID of the device to generate a unique path."),(0,r.kt)("li",{parentName:"ul"},"For I/O isolation, a dedicated physical disk per PV is recommended to ensure hardware-based isolation."),(0,r.kt)("li",{parentName:"ul"},"For capacity isolation, a partition per PV or a physical disk per PV is recommended.")),(0,r.kt)("p",null,"Refer to ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/best-practices.md"},"Best Practices")," for more information on local PV in Kubernetes."),(0,r.kt)("h2",{id:"disk-mount-examples"},"Disk mount examples"),(0,r.kt)("p",null,"If the components such as monitoring, TiDB Binlog, and ",(0,r.kt)("inlineCode",{parentName:"p"},"tidb-backup")," use local disks to store data, you can mount SAS disks and create separate ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for them to use. Procedures are as follows:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For a disk storing monitoring data, follow the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#sharing-a-disk-filesystem-by-multiple-filesystem-pvs"},"steps")," to mount the disk. First, create multiple directories in the disk, and bind mount them into ",(0,r.kt)("inlineCode",{parentName:"p"},"/mnt/disks")," directory. Then, create ",(0,r.kt)("inlineCode",{parentName:"p"},"local-storage")," ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for them to use."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"The number of directories you create depends on the planned number of TiDB clusters. For each directory, a corresponding PV will be created. The monitoring data in each TiDB cluster uses one PV."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For a disk storing TiDB Binlog and backup data, follow the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#sharing-a-disk-filesystem-by-multiple-filesystem-pvs"},"steps")," to mount the disk. First, create multiple directories in the disk, and bind mount them into ",(0,r.kt)("inlineCode",{parentName:"p"},"/mnt/backup")," directory. Then, create ",(0,r.kt)("inlineCode",{parentName:"p"},"backup-storage")," ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for them to use."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"The number of directories you create depends on the planned number of TiDB clusters, the number of Pumps in each cluster, and your backup method. For each directory, a corresponding PV will be created. Each Pump uses one PV and each Drainer uses one PV. All ",(0,r.kt)("a",{parentName:"p",href:"/backup-to-s3#ad-hoc-full-backup-to-s3-compatible-storage"},"Ad-hoc full backup")," tasks and all ",(0,r.kt)("a",{parentName:"p",href:"/backup-to-s3#scheduled-full-backup-to-s3-compatible-storage"},"scheduled full backup")," tasks share one PV."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For a disk storing data in PD, follow the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#sharing-a-disk-filesystem-by-multiple-filesystem-pvs"},"steps")," to mount the disk. First, create multiple directories in the disk, and bind mount them into ",(0,r.kt)("inlineCode",{parentName:"p"},"/mnt/sharedssd")," directory. Then, create ",(0,r.kt)("inlineCode",{parentName:"p"},"shared-ssd-storage")," ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for them to use."),(0,r.kt)("blockquote",{parentName:"li"},(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("p",{parentName:"blockquote"},"The number of directories you create depends on the planned number of TiDB clusters, and the number of PD servers in each cluster. For each directory, a corresponding PV will be created. Each PD server uses one PV."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For a disk storing data in TiKV, you can ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#use-a-whole-disk-as-a-filesystem-pv"},"mount")," it into ",(0,r.kt)("inlineCode",{parentName:"p"},"/mnt/ssd")," directory, and create ",(0,r.kt)("inlineCode",{parentName:"p"},"ssd-storage")," ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for it to use."))),(0,r.kt)("p",null,"Based on the disk mounts above, you need to modify the ",(0,r.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/local-dind/local-volume-provisioner.yaml"},(0,r.kt)("inlineCode",{parentName:"a"},"local-volume-provisioner")," YAML file")," accordingly, configure discovery directory and create the necessary ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass"),". Here is an example of a modified YAML file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: "local-storage"\nprovisioner: "kubernetes.io/no-provisioner"\nvolumeBindingMode: "WaitForFirstConsumer"\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: "ssd-storage"\nprovisioner: "kubernetes.io/no-provisioner"\nvolumeBindingMode: "WaitForFirstConsumer"\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: "shared-ssd-storage"\nprovisioner: "kubernetes.io/no-provisioner"\nvolumeBindingMode: "WaitForFirstConsumer"\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: "backup-storage"\nprovisioner: "kubernetes.io/no-provisioner"\nvolumeBindingMode: "WaitForFirstConsumer"\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: local-provisioner-config\n  namespace: kube-system\ndata:\n  nodeLabelsForPV: |\n    - kubernetes.io/hostname\n  storageClassMap: |\n    shared-ssd-storage:\n      hostDir: /mnt/sharedssd\n      mountDir: /mnt/sharedssd\n    ssd-storage:\n      hostDir: /mnt/ssd\n      mountDir: /mnt/ssd\n    local-storage:\n      hostDir: /mnt/disks\n      mountDir: /mnt/disks\n    backup-storage:\n      hostDir: /mnt/backup\n      mountDir: /mnt/backup\n---\n\n......\n\n          volumeMounts:\n\n            ......\n\n            - mountPath: /mnt/ssd\n              name: local-ssd\n              mountPropagation: "HostToContainer"\n            - mountPath: /mnt/sharedssd\n              name: local-sharedssd\n              mountPropagation: "HostToContainer"\n            - mountPath: /mnt/disks\n              name: local-disks\n              mountPropagation: "HostToContainer"\n            - mountPath: /mnt/backup\n              name: local-backup\n              mountPropagation: "HostToContainer"\n      volumes:\n\n        ......\n\n        - name: local-ssd\n          hostPath:\n            path: /mnt/ssd\n        - name: local-sharedssd\n          hostPath:\n            path: /mnt/sharedssd\n        - name: local-disks\n          hostPath:\n            path: /mnt/disks\n        - name: local-backup\n          hostPath:\n            path: /mnt/backup\n......\n\n')),(0,r.kt)("p",null,"Finally, execute the ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl apply")," command to deploy ",(0,r.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/local-dind/local-volume-provisioner.yaml\n")),(0,r.kt)("p",null,"When you later deploy tidb clusters, deploy TiDB Binlog for incremental backups, or do full backups, configure the corresponding ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for use."),(0,r.kt)("h2",{id:"data-safety"},"Data safety"),(0,r.kt)("p",null,"In general, after a PVC is no longer used and deleted, the PV bound to it is reclaimed and placed in the resource pool for scheduling by the provisioner. To avoid accidental data loss, you can globally configure the reclaim policy of the ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain")," or only change the reclaim policy of a single PV to ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain"),". With the ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain")," policy, a PV is not automatically reclaimed."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Configure globally:"),(0,r.kt)("p",{parentName:"li"},"  The reclaim policy of a ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," is set at creation time and it cannot be updated once it is created. If it is not set when created, you can create another ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," of the same provisioner. For example, the default reclaim policy of the ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," for persistent disks on Google Kubernetes Engine (GKE) is ",(0,r.kt)("inlineCode",{parentName:"p"},"Delete"),". You can create another ",(0,r.kt)("inlineCode",{parentName:"p"},"StorageClass")," named ",(0,r.kt)("inlineCode",{parentName:"p"},"pd-standard")," with its reclaim policy as ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain"),", and change the ",(0,r.kt)("inlineCode",{parentName:"p"},"storageClassName")," of the corresponding component to ",(0,r.kt)("inlineCode",{parentName:"p"},"pd-standard")," when creating a TiDB cluster."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: pd-standard\nparameters:\n   type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Configure a single PV:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl patch pv ${pv_name} -p \'{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}\'\n')))),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"By default, to ensure data safety, TiDB Operator automatically changes the reclaim policy of the PVs of PD and TiKV to ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain"),"."))),(0,r.kt)("h3",{id:"delete-pv-and-data"},"Delete PV and data"),(0,r.kt)("p",null,"When the reclaim policy of PVs is set to ",(0,r.kt)("inlineCode",{parentName:"p"},"Retain"),", if you have confirmed that the data of a PV can be deleted, you can delete this PV and the corresponding data by strictly taking the following steps:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Delete the PVC object corresponding to the PV:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl delete pvc ${pvc_name} --namespace=${namespace}\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Set the reclaim policy of the PV to ",(0,r.kt)("inlineCode",{parentName:"p"},"Delete"),". Then the PV is automatically deleted and reclaimed."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl patch pv ${pv_name} -p \'{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}\'\n')))),(0,r.kt)("p",null,"For more details, refer to ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/"},"Change the Reclaim Policy of a PersistentVolume"),"."))}u.isMDXComponent=!0}}]);