"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[8877],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=o,h=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return n?a.createElement(h,r(r({ref:t},c),{},{components:n})):a.createElement(h,r({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},9969:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},assets:function(){return c},toc:function(){return d},default:function(){return m}});var a=n(7462),o=n(3366),i=(n(7294),n(3905)),r=["components"],l={title:"Configure a TiDB Cluster in Kubernetes",summary:"Learn how to configure a TiDB cluster in Kubernetes."},s="Configure a TiDB Cluster in Kubernetes",p={unversionedId:"configure-a-tidb-cluster",id:"configure-a-tidb-cluster",title:"Configure a TiDB Cluster in Kubernetes",description:"This document introduces how to configure a TiDB cluster for production deployment. It covers the following content:",source:"@site/docs/configure-a-tidb-cluster.md",sourceDirName:".",slug:"/configure-a-tidb-cluster",permalink:"/docusaurus-operator/configure-a-tidb-cluster",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/configure-a-tidb-cluster.md",tags:[],version:"current",frontMatter:{title:"Configure a TiDB Cluster in Kubernetes",summary:"Learn how to configure a TiDB cluster in Kubernetes."},sidebar:"mySidebar",previous:{title:"Deploy TiDB Operator in Kubernetes",permalink:"/docusaurus-operator/deploy-tidb-operator"},next:{title:"Deploy TiDB in General Kubernetes",permalink:"/docusaurus-operator/deploy-on-general-kubernetes"}},c={},d=[{value:"Configure resources",id:"configure-resources",level:2},{value:"Configure TiDB deployment",id:"configure-tidb-deployment",level:2},{value:"Cluster name",id:"cluster-name",level:3},{value:"Version",id:"version",level:3},{value:"Recommended configuration",id:"recommended-configuration",level:3},{value:"configUpdateStrategy",id:"configupdatestrategy",level:4},{value:"enableDynamicConfiguration",id:"enabledynamicconfiguration",level:4},{value:"pvReclaimPolicy",id:"pvreclaimpolicy",level:4},{value:"mountClusterClientSecret",id:"mountclusterclientsecret",level:4},{value:"Storage",id:"storage",level:3},{value:"Storage Class",id:"storage-class",level:4},{value:"Multiple disks mounting",id:"multiple-disks-mounting",level:4},{value:"HostNetwork",id:"hostnetwork",level:3},{value:"Discovery",id:"discovery",level:3},{value:"Cluster topology",id:"cluster-topology",level:3},{value:"PD/TiKV/TiDB",id:"pdtikvtidb",level:4},{value:"Enable TiFlash",id:"enable-tiflash",level:4},{value:"Enable TiCDC",id:"enable-ticdc",level:4},{value:"Deploy Enterprise Edition",id:"deploy-enterprise-edition",level:4},{value:"Configure TiDB components",id:"configure-tidb-components",level:3},{value:"Configure TiDB parameters",id:"configure-tidb-parameters",level:4},{value:"Configure TiKV parameters",id:"configure-tikv-parameters",level:4},{value:"Configure PD parameters",id:"configure-pd-parameters",level:4},{value:"Configure TiFlash parameters",id:"configure-tiflash-parameters",level:4},{value:"Configure TiCDC start parameters",id:"configure-ticdc-start-parameters",level:4},{value:"Configure automatic failover thresholds of PD, TiDB, TiKV, and TiFlash",id:"configure-automatic-failover-thresholds-of-pd-tidb-tikv-and-tiflash",level:4},{value:"Configure graceful upgrade for TiDB cluster",id:"configure-graceful-upgrade-for-tidb-cluster",level:3},{value:"Configure graceful upgrade for TiKV cluster",id:"configure-graceful-upgrade-for-tikv-cluster",level:3},{value:"Configure PV for TiDB slow logs",id:"configure-pv-for-tidb-slow-logs",level:3},{value:"Configure using <code>spec.tidb.storageVolumes</code>",id:"configure-using-spectidbstoragevolumes",level:4},{value:"Configure using <code>spec.tidb.additionalVolumes</code>",id:"configure-using-spectidbadditionalvolumes",level:4},{value:"Configure TiDB service",id:"configure-tidb-service",level:3},{value:"ClusterIP",id:"clusterip",level:4},{value:"NodePort",id:"nodeport",level:4},{value:"LoadBalancer",id:"loadbalancer",level:4},{value:"Configure high availability",id:"configure-high-availability",level:2},{value:"High availability of TiDB service",id:"high-availability-of-tidb-service",level:3},{value:"Use affinity to schedule pods",id:"use-affinity-to-schedule-pods",level:4},{value:"Use topologySpreadConstraints to make pods evenly spread",id:"use-topologyspreadconstraints-to-make-pods-evenly-spread",level:4},{value:"High availability of data",id:"high-availability-of-data",level:3}],u={toc:d};function m(e){var t=e.components,n=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"configure-a-tidb-cluster-in-kubernetes"},"Configure a TiDB Cluster in Kubernetes"),(0,i.kt)("p",null,"This document introduces how to configure a TiDB cluster for production deployment. It covers the following content:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"#configure-resources"},"Configure resources"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"#configure-tidb-deployment"},"Configure TiDB deployment"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"#configure-high-availability"},"Configure high availability")))),(0,i.kt)("h2",{id:"configure-resources"},"Configure resources"),(0,i.kt)("p",null,"Before deploying a TiDB cluster, it is necessary to configure the resources for each component of the cluster depending on your needs. PD, TiKV, and TiDB are the core service components of a TiDB cluster. In a production environment, you need to configure resources of these components according to their needs. For details, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/hardware-and-software-requirements/"},"Hardware Recommendations"),"."),(0,i.kt)("p",null,"To ensure the proper scheduling and stable operation of the components of the TiDB cluster in Kubernetes, it is recommended to set Guaranteed-level quality of service (QoS) by making ",(0,i.kt)("inlineCode",{parentName:"p"},"limits")," equal to ",(0,i.kt)("inlineCode",{parentName:"p"},"requests")," when configuring resources. For details, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/"},"Configure Quality of Service for Pods"),"."),(0,i.kt)("p",null,"If you are using a NUMA-based CPU, you need to enable ",(0,i.kt)("inlineCode",{parentName:"p"},"Static"),"'s CPU management policy on the node for better performance. In order to allow the TiDB cluster component to monopolize the corresponding CPU resources, the CPU quota must be an integer greater than or equal to ",(0,i.kt)("inlineCode",{parentName:"p"},"1"),", apart from setting Guaranteed-level QoS as mentioned above. For details, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies"},"Control CPU Management Policies on the Node"),"."),(0,i.kt)("h2",{id:"configure-tidb-deployment"},"Configure TiDB deployment"),(0,i.kt)("p",null,"To configure a TiDB deployment, you need to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"TiDBCluster")," CR. Refer to the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pingcap/tidb-operator/blob/master/examples/advanced/tidb-cluster.yaml"},"TidbCluster example")," for an example. For the complete configurations of ",(0,i.kt)("inlineCode",{parentName:"p"},"TiDBCluster")," CR, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pingcap/tidb-operator/blob/master/docs/api-references/docs.md"},"API documentation"),"."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"It is recommended to organize configurations for a TiDB cluster under a directory of ",(0,i.kt)("inlineCode",{parentName:"p"},"cluster_name")," and save it as ",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}/tidb-cluster.yaml"),"."))),(0,i.kt)("p",null,"The modified configuration is not automatically applied to the TiDB cluster by default. The new configuration file is loaded only when the Pod restarts."),(0,i.kt)("h3",{id:"cluster-name"},"Cluster name"),(0,i.kt)("p",null,"The cluster name can be configured by changing ",(0,i.kt)("inlineCode",{parentName:"p"},"metadata.name")," in the ",(0,i.kt)("inlineCode",{parentName:"p"},"TiDBCuster")," CR."),(0,i.kt)("h3",{id:"version"},"Version"),(0,i.kt)("p",null,"Usually, components in a cluster are in the same version. It is recommended to configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.<pd/tidb/tikv/pump/tiflash/ticdc>.baseImage")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.version"),", if you need to configure different versions for different components, you can configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.<pd/tidb/tikv/pump/tiflash/ticdc>.version"),"."),(0,i.kt)("p",null,"Here are the formats of the parameters:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"spec.version"),": the format is ",(0,i.kt)("inlineCode",{parentName:"p"},"imageTag"),", such as ",(0,i.kt)("inlineCode",{parentName:"p"},"v5.3.0"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"spec.<pd/tidb/tikv/pump/tiflash/ticdc>.baseImage"),": the format is ",(0,i.kt)("inlineCode",{parentName:"p"},"imageName"),", such as ",(0,i.kt)("inlineCode",{parentName:"p"},"pingcap/tidb"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"spec.<pd/tidb/tikv/pump/tiflash/ticdc>.version"),": the format is ",(0,i.kt)("inlineCode",{parentName:"p"},"imageTag"),", such as ",(0,i.kt)("inlineCode",{parentName:"p"},"v5.3.0")))),(0,i.kt)("h3",{id:"recommended-configuration"},"Recommended configuration"),(0,i.kt)("h4",{id:"configupdatestrategy"},"configUpdateStrategy"),(0,i.kt)("p",null,"It is recommended that you configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.configUpdateStrategy: RollingUpdate")," to enable automatic update of configurations. This way, every time the configuration is updated, all components are rolling updated automatically, and the modified configuration is applied to the cluster."),(0,i.kt)("h4",{id:"enabledynamicconfiguration"},"enableDynamicConfiguration"),(0,i.kt)("p",null,"It is recommended that you configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.enableDynamicConfiguration: true")," to enable the ",(0,i.kt)("inlineCode",{parentName:"p"},"--advertise-status-addr")," startup parameter for TiKV."),(0,i.kt)("p",null,"Versions required:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"TiDB 4.0.1 or later versions")),(0,i.kt)("h4",{id:"pvreclaimpolicy"},"pvReclaimPolicy"),(0,i.kt)("p",null,"It is recommended that you configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.pvReclaimPolicy: Retain")," to ensure that the PV is retained even if the PVC is deleted. This is to ensure your data safety."),(0,i.kt)("h4",{id:"mountclusterclientsecret"},"mountClusterClientSecret"),(0,i.kt)("p",null,"PD and TiKV supports configuring ",(0,i.kt)("inlineCode",{parentName:"p"},"mountClusterClientSecret"),". If ",(0,i.kt)("a",{parentName:"p",href:"/docusaurus-operator/enable-tls-between-components"},"TLS is enabled between cluster components"),", it is recommended to configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.pd.mountClusterClientSecret: true")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tikv.mountClusterClientSecret: true"),". Under such configuration, TiDB Operator automatically mounts the ",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}-cluster-client-secret")," certificate to the PD and TiKV container, so you can conveniently ",(0,i.kt)("a",{parentName:"p",href:"/docusaurus-operator/enable-tls-between-components#configure-pd-ctl-tikv-ctl-and-connect-to-the-cluster"},"use ",(0,i.kt)("inlineCode",{parentName:"a"},"pd-ctl")," and ",(0,i.kt)("inlineCode",{parentName:"a"},"tikv-ctl")),"."),(0,i.kt)("h3",{id:"storage"},"Storage"),(0,i.kt)("h4",{id:"storage-class"},"Storage Class"),(0,i.kt)("p",null,"You can set the storage class by modifying ",(0,i.kt)("inlineCode",{parentName:"p"},"storageClassName")," of each component in ",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}/tidb-cluster.yaml")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}/tidb-monitor.yaml"),". For the ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/storage-classes/"},"storage classes")," supported by the Kubernetes cluster, check with your system administrator."),(0,i.kt)("p",null,"Different components of a TiDB cluster have different disk requirements. Before deploying a TiDB cluster, refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docusaurus-operator/configure-storage-class"},"Storage Configuration document")," to select an appropriate storage class for each component according to the storage classes supported by the current Kubernetes cluster and usage scenario."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"When you create the TiDB cluster, if you set a storage class that does not exist in the Kubernetes cluster, then the TiDB cluster creation goes to the Pending state. In this situation, you must ",(0,i.kt)("a",{parentName:"p",href:"/docusaurus-operator/destroy-a-tidb-cluster"},"destroy the TiDB cluster in Kubernetes")," and retry the creation."))),(0,i.kt)("h4",{id:"multiple-disks-mounting"},"Multiple disks mounting"),(0,i.kt)("p",null,"TiDB Operator supports mounting multiple PVs for PD, TiDB, TiKV, and TiCDC, which can be used for data writing for different purposes."),(0,i.kt)("p",null,"You can configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolumes")," field for each component to describe multiple user-customized PVs."),(0,i.kt)("p",null,"The meanings of the related fields are as follows:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"storageVolume.name"),": The name of the PV."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"storageVolume.storageClassName"),": The StorageClass that the PV uses. If not configured, ",(0,i.kt)("inlineCode",{parentName:"li"},"spec.pd/tidb/tikv.storageClassName")," will be used."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"storageVolume.storageSize"),": The storage size of the requested PV."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"storageVolume.mountPath"),": The path of the container to mount the PV to.")),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'  tikv:\n    ...\n    config: |\n      [rocksdb]\n        wal-dir = "/data_sbi/tikv/wal"\n      [titan]\n        dirname = "/data_sbj/titan/data"\n    storageVolumes:\n    - name: wal\n      storageSize: "2Gi"\n      mountPath: "/data_sbi/tikv/wal"\n    - name: titan\n      storageSize: "2Gi"\n      mountPath: "/data_sbj/titan/data"\n')),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"TiDB Operator uses some mount paths by default. For example, it mounts ",(0,i.kt)("inlineCode",{parentName:"p"},"EmptyDir")," to the ",(0,i.kt)("inlineCode",{parentName:"p"},"/var/log/tidb")," directory for the TiDB Pod. Therefore, avoid duplicate ",(0,i.kt)("inlineCode",{parentName:"p"},"mountPath")," when you configure ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolumes"),"."))),(0,i.kt)("h3",{id:"hostnetwork"},"HostNetwork"),(0,i.kt)("p",null,"For PD, TiKV, TiDB, TiFlash, TiCDC, and Pump, you can configure the Pods to use the host namespace ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces"},(0,i.kt)("inlineCode",{parentName:"a"},"HostNetwork")),"."),(0,i.kt)("p",null,"To enable ",(0,i.kt)("inlineCode",{parentName:"p"},"HostNetwork")," for all supported components, configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.hostNetwork: true"),"."),(0,i.kt)("p",null,"To enable ",(0,i.kt)("inlineCode",{parentName:"p"},"HostNetwork")," for specified components, configure ",(0,i.kt)("inlineCode",{parentName:"p"},"hostNetwork: true")," for the components."),(0,i.kt)("h3",{id:"discovery"},"Discovery"),(0,i.kt)("p",null,"TiDB Operator starts a Discovery service for each TiDB cluster. The Discovery service can return the corresponding startup parameters for each PD Pod to support the startup of the PD cluster. You can configure resources of the Discovery service using ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.discovery"),". For details, see ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"},"Managing Resources for Containers"),"."),(0,i.kt)("p",null,"A ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.discovery")," configuration example is as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  discovery:\n    limits:\n      cpu: "0.2"\n    requests:\n      cpu: "0.2"\n  ...\n')),(0,i.kt)("h3",{id:"cluster-topology"},"Cluster topology"),(0,i.kt)("h4",{id:"pdtikvtidb"},"PD/TiKV/TiDB"),(0,i.kt)("p",null,"The deployed cluster topology by default has three PD Pods, three TiKV Pods, and two TiDB Pods. In this deployment topology, the scheduler extender of TiDB Operator requires at least three nodes in the Kubernetes cluster to provide high availability. You can modify the ",(0,i.kt)("inlineCode",{parentName:"p"},"replicas")," configuration to change the number of pods for each component."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If the number of Kubernetes cluster nodes is less than three, one PD Pod goes to the Pending state, and neither TiKV Pods nor TiDB Pods are created. When the number of nodes in the Kubernetes cluster is less than three, to start the TiDB cluster, you can reduce the number of PD Pods in the default deployment to ",(0,i.kt)("inlineCode",{parentName:"p"},"1"),"."))),(0,i.kt)("h4",{id:"enable-tiflash"},"Enable TiFlash"),(0,i.kt)("p",null,"If you want to enable TiFlash in the cluster, configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.pd.config.replication.enable-placement-rules: true")," and configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tiflash")," in the ",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}/tidb-cluster.yaml")," file as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"  pd:\n    config: |\n      ...\n      [replication]\n      enable-placement-rules = true\n  tiflash:\n    baseImage: pingcap/tiflash\n    maxFailoverCount: 0\n    replicas: 1\n    storageClaims:\n    - resources:\n        requests:\n          storage: 100Gi\n      storageClassName: local-storage\n")),(0,i.kt)("p",null,"TiFlash supports mounting multiple Persistent Volumes (PVs). If you want to configure multiple PVs for TiFlash, configure multiple ",(0,i.kt)("inlineCode",{parentName:"p"},"resources")," in ",(0,i.kt)("inlineCode",{parentName:"p"},"tiflash.storageClaims"),", each ",(0,i.kt)("inlineCode",{parentName:"p"},"resources")," with a separate ",(0,i.kt)("inlineCode",{parentName:"p"},"storage request")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"storageClassName"),". For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"  tiflash:\n    baseImage: pingcap/tiflash\n    maxFailoverCount: 0\n    replicas: 1\n    storageClaims:\n    - resources:\n        requests:\n          storage: 100Gi\n      storageClassName: local-storage\n    - resources:\n        requests:\n          storage: 100Gi\n      storageClassName: local-storage\n")),(0,i.kt)("p",null,"TiFlash mounts all PVs to directories such as ",(0,i.kt)("inlineCode",{parentName:"p"},"/data0")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"/data1")," in the container in the order of configuration. TiFlash has four log files. The proxy log is printed in the standard output of the container. The other three logs are stored in the disk under the ",(0,i.kt)("inlineCode",{parentName:"p"},"/data0")," directory by default, which are ",(0,i.kt)("inlineCode",{parentName:"p"},"/data0/logs/flash_cluster_manager.log"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"/ data0/logs/error.log"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"/data0/logs/server.log"),". To modify the log storage path, refer to ",(0,i.kt)("a",{parentName:"p",href:"#configure-tiflash-parameters"},"Configure TiFlash parameters"),"."),(0,i.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Since TiDB Operator will mount PVs automatically in the ",(0,i.kt)("strong",{parentName:"p"},"order")," of the items in the ",(0,i.kt)("inlineCode",{parentName:"p"},"storageClaims")," list, if you need to add more disks to TiFlash, make sure to append the new item only to the ",(0,i.kt)("strong",{parentName:"p"},"end")," of the original items, and ",(0,i.kt)("strong",{parentName:"p"},"DO NOT")," modify the order of the original items."))),(0,i.kt)("h4",{id:"enable-ticdc"},"Enable TiCDC"),(0,i.kt)("p",null,"If you want to enable TiCDC in the cluster, you can add TiCDC spec to the ",(0,i.kt)("inlineCode",{parentName:"p"},"TiDBCluster")," CR. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"  spec:\n    ticdc:\n      baseImage: pingcap/ticdc\n      replicas: 3\n")),(0,i.kt)("h4",{id:"deploy-enterprise-edition"},"Deploy Enterprise Edition"),(0,i.kt)("p",null,"To deploy Enterprise Edition of TiDB/PD/TiKV/TiFlash/TiCDC, edit the ",(0,i.kt)("inlineCode",{parentName:"p"},"db.yaml")," file to set ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.<tidb/pd/tikv/tiflash/ticdc>.baseImage")," to the enterprise image (",(0,i.kt)("inlineCode",{parentName:"p"},"pingcap/<tidb/pd/tikv/tiflash/ticdc>-enterprise"),")."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  pd:\n    baseImage: pingcap/pd-enterprise\n  ...\n  tikv:\n    baseImage: pingcap/tikv-enterprise\n")),(0,i.kt)("h3",{id:"configure-tidb-components"},"Configure TiDB components"),(0,i.kt)("p",null,"This section introduces how to configure the parameters of TiDB/TiKV/PD/TiFlash/TiCDC."),(0,i.kt)("h4",{id:"configure-tidb-parameters"},"Configure TiDB parameters"),(0,i.kt)("p",null,"TiDB parameters can be configured by ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.config")," in TidbCluster Custom Resource."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tidb:\n    config: |\n      split-table = true\n      oom-action = "log"\n')),(0,i.kt)("p",null,"For all the configurable parameters of TiDB, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/reference/configuration/tidb-server/configuration-file/"},"TiDB Configuration File"),"."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If you deploy your TiDB cluster using CR, make sure that ",(0,i.kt)("inlineCode",{parentName:"p"},"Config: {}")," is set, no matter you want to modify ",(0,i.kt)("inlineCode",{parentName:"p"},"config")," or not. Otherwise, TiDB components might not be started successfully. This step is meant to be compatible with ",(0,i.kt)("inlineCode",{parentName:"p"},"Helm")," deployment."))),(0,i.kt)("h4",{id:"configure-tikv-parameters"},"Configure TiKV parameters"),(0,i.kt)("p",null,"TiKV parameters can be configured by ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tikv.config")," in TidbCluster Custom Resource."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tikv:\n    config: |\n      [storage]\n        [storage.block-cache]\n          capacity = "16GB"\n')),(0,i.kt)("p",null,"For all the configurable parameters of TiKV, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/reference/configuration/tikv-server/configuration-file/"},"TiKV Configuration File"),"."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If you deploy your TiDB cluster using CR, make sure that ",(0,i.kt)("inlineCode",{parentName:"p"},"Config: {}")," is set, no matter you want to modify ",(0,i.kt)("inlineCode",{parentName:"p"},"config")," or not. Otherwise, TiKV components might not be started successfully. This step is meant to be compatible with ",(0,i.kt)("inlineCode",{parentName:"p"},"Helm")," deployment."))),(0,i.kt)("h4",{id:"configure-pd-parameters"},"Configure PD parameters"),(0,i.kt)("p",null,"PD parameters can be configured by ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.pd.config")," in TidbCluster Custom Resource."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  pd:\n    config: |\n      lease = 3\n      enable-prevote = true\n")),(0,i.kt)("p",null,"For all the configurable parameters of PD, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/reference/configuration/pd-server/configuration-file/"},"PD Configuration File"),"."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("ul",{parentName:"div"},(0,i.kt)("li",{parentName:"ul"},"If you deploy your TiDB cluster using CR, make sure that ",(0,i.kt)("inlineCode",{parentName:"li"},"Config: {}")," is set, no matter you want to modify ",(0,i.kt)("inlineCode",{parentName:"li"},"config")," or not. Otherwise, PD components might not be started successfully. This step is meant to be compatible with ",(0,i.kt)("inlineCode",{parentName:"li"},"Helm")," deployment."),(0,i.kt)("li",{parentName:"ul"},"After the cluster is started for the first time, some PD configuration items are persisted in etcd. The persisted configuration in etcd takes precedence over that in PD. Therefore, after the first start, you cannot modify some PD configuration using parameters. You need to dynamically modify the configuration using SQL statements, pd-ctl, or PD server API. Currently, among all the configuration items listed in ",(0,i.kt)("a",{parentName:"li",href:"https://docs.pingcap.com/tidb/stable/dynamic-config#modify-pd-configuration-online"},"Modify PD configuration online"),", except ",(0,i.kt)("inlineCode",{parentName:"li"},"log.level"),", all the other configuration items cannot be modified using parameters after the first start.")))),(0,i.kt)("h4",{id:"configure-tiflash-parameters"},"Configure TiFlash parameters"),(0,i.kt)("p",null,"TiFlash parameters can be configured by ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tiflash.config")," in TidbCluster Custom Resource."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tiflash:\n    config:\n      config: |\n        [flash]\n          [flash.flash_cluster]\n            log = "/data0/logs/flash_cluster_manager.log"\n        [logger]\n          count = 10\n          level = "information"\n          errorlog = "/data0/logs/error.log"\n          log = "/data0/logs/server.log"\n')),(0,i.kt)("p",null,"For all the configurable parameters of TiFlash, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/tiflash/tiflash-configuration/"},"TiFlash Configuration File"),"."),(0,i.kt)("h4",{id:"configure-ticdc-start-parameters"},"Configure TiCDC start parameters"),(0,i.kt)("p",null,"You can configure TiCDC start parameters through ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.ticdc.config")," in TidbCluster Custom Resource."),(0,i.kt)("p",null,"For example:"),(0,i.kt)("p",null,"For TiDB Operator v1.2.0-rc.2 and later versions, configure the parameters in the TOML format as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  ticdc:\n    config: |\n      gc-ttl = 86400\n      log-level = "info"\n')),(0,i.kt)("p",null,"For TiDB Operator versions earlier than v1.2.0-rc.2, configure the parameters in the YAML format as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ticdc:\n    config:\n      timezone: UTC\n      gcTTL: 86400\n      logLevel: info\n")),(0,i.kt)("p",null,"For all configurable start parameters of TiCDC, see ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pingcap/tiflow/blob/bf29e42c75ae08ce74fbba102fe78a0018c9d2ea/pkg/cmd/util/ticdc.toml"},"TiCDC configuration"),"."),(0,i.kt)("h4",{id:"configure-automatic-failover-thresholds-of-pd-tidb-tikv-and-tiflash"},"Configure automatic failover thresholds of PD, TiDB, TiKV, and TiFlash"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"/docusaurus-operator/use-auto-failover"},"automatic failover")," feature is enabled by default in TiDB Operator. When the Pods of PD, TiDB, TiKV, TiFlash fail or the corresponding nodes fail, TiDB Operator performs failover automatically and replenish the number of Pod replicas by scaling the corresponding components."),(0,i.kt)("p",null,"To avoid that the automatic failover feature creates too many Pods, you can configure the threshold of the maximum number of Pods that TiDB Operator can create during failover for each component. The default threshold is ",(0,i.kt)("inlineCode",{parentName:"p"},"3"),". If the threshold for a component is configured to ",(0,i.kt)("inlineCode",{parentName:"p"},"0"),", it means that the automatic failover feature is disabled for this component. An example configuration is as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"  pd:\n    maxFailoverCount: 3\n  tidb:\n    maxFailoverCount: 3\n  tikv:\n    maxFailoverCount: 3\n  tiflash:\n    maxFailoverCount: 3\n")),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"For the following cases, configure ",(0,i.kt)("inlineCode",{parentName:"p"},"maxFailoverCount: 0")," explicitly:"),(0,i.kt)("ul",{parentName:"div"},(0,i.kt)("li",{parentName:"ul"},"The Kubernetes cluster does not have enough resources for TiDB Operator to scale out the new Pod. In such cases, the new Pod will be in the Pending state."),(0,i.kt)("li",{parentName:"ul"},"You do not want to enable the automatic failover function.")))),(0,i.kt)("h3",{id:"configure-graceful-upgrade-for-tidb-cluster"},"Configure graceful upgrade for TiDB cluster"),(0,i.kt)("p",null,"When you perform a rolling update to the TiDB cluster, Kubernetes sends a ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods"},(0,i.kt)("inlineCode",{parentName:"a"},"TERM"))," signal to the TiDB server before it stops the TiDB Pod. When the TiDB server receives the ",(0,i.kt)("inlineCode",{parentName:"p"},"TERM")," signal, it tries to wait for all connections to close. After 15 seconds, the TiDB server forcibly closes all the connections and exits the process."),(0,i.kt)("p",null,"You can enable this feature by configuring the following items:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"spec.tidb.terminationGracePeriodSeconds"),": The longest tolerable duration to delete the old TiDB Pod during the rolling upgrade. If this duration is exceeded, the TiDB Pod will be deleted forcibly."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"spec.tidb.lifecycle"),": Sets the ",(0,i.kt)("inlineCode",{parentName:"li"},"preStop")," hook for the TiDB Pod, which is the operation executed before the TiDB server stops.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  tidb:\n    terminationGracePeriodSeconds: 60\n    lifecycle:\n      preStop:\n        exec:\n          command:\n          - /bin/sh\n          - -c\n          - "sleep 10 && kill -QUIT 1"\n')),(0,i.kt)("p",null,"The YAML file above:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Sets the longest tolerable duration to delete the TiDB Pod to 60 seconds. If the client does not close the connections after 60 seconds, these connections will be closed forcibly. You can adjust the value according to your needs."),(0,i.kt)("li",{parentName:"ul"},"Sets the value of ",(0,i.kt)("inlineCode",{parentName:"li"},"preStop")," hook to ",(0,i.kt)("inlineCode",{parentName:"li"},"sleep 10 && kill -QUIT 1"),". Here ",(0,i.kt)("inlineCode",{parentName:"li"},"PID 1")," refers to the PID of the TiDB server process in the TiDB Pod. When the TiDB server process receives the signal, it exits only after all the connections are closed by the client.")),(0,i.kt)("p",null,"When Kubernetes deletes the TiDB Pod, it also removes the TiDB node from the service endpoints. This is to ensure that the new connection is not established to this TiDB node. However, because this process is asynchronous, you can make the system sleep for a few seconds before you send the ",(0,i.kt)("inlineCode",{parentName:"p"},"kill")," signal, which makes sure that the TiDB node is removed from the endpoints."),(0,i.kt)("h3",{id:"configure-graceful-upgrade-for-tikv-cluster"},"Configure graceful upgrade for TiKV cluster"),(0,i.kt)("p",null,"During TiKV upgrade, TiDB Operator evicts all Region leaders from TiKV Pod before restarting TiKV Pod. Only after the eviction is completed (which means the number of Region leaders on TiKV Pod drops to 0) or the eviction exceeds the specified timeout (10 minutes by default), TiKV Pod is restarted."),(0,i.kt)("p",null,"If the eviction of Region leaders exceeds the specified timeout, restarting TiKV Pod causes issues such as failures of some requests or more latency. To avoid the issues, you can configure the timeout ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tikv.evictLeaderTimeout")," (10 minutes by default) to a larger value. For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"spec:\n  tikv:\n    evictLeaderTimeout: 10000m\n")),(0,i.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If the TiKV version is earlier than 4.0.14 or 5.0.3, due to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/tikv/tikv/pull/10364"},"a bug of TiKV"),", you need to configure the timeout ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tikv.evictLeaderTimeout")," as large as possible to ensure that all Region leaders on the TiKV Pod can be evicted within the timeout. If you are not sure about the proper value, greater than '1500m' is recommended."))),(0,i.kt)("h3",{id:"configure-pv-for-tidb-slow-logs"},"Configure PV for TiDB slow logs"),(0,i.kt)("p",null,"By default, TiDB Operator creates a ",(0,i.kt)("inlineCode",{parentName:"p"},"slowlog")," volume (which is an ",(0,i.kt)("inlineCode",{parentName:"p"},"EmptyDir"),") to store the slow logs, mounts the ",(0,i.kt)("inlineCode",{parentName:"p"},"slowlog")," volume to ",(0,i.kt)("inlineCode",{parentName:"p"},"/var/log/tidb"),", and prints slow logs in the ",(0,i.kt)("inlineCode",{parentName:"p"},"stdout")," through a sidecar container."),(0,i.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"By default, after a Pod is deleted (for example, rolling update), the slow query logs stored using the ",(0,i.kt)("inlineCode",{parentName:"p"},"EmptyDir")," volume are lost. Make sure that a log collection solution has been deployed in the Kubernetes cluster to collect logs of all containers. If you do not deploy such a log collection solution, you ",(0,i.kt)("strong",{parentName:"p"},"must")," make the following configuration to use a persistent volume to store the slow query logs."))),(0,i.kt)("p",null,"If you want to use a separate PV to store the slow logs, you can specify the name of the PV in ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.slowLogVolumeName"),", and then configure the PV in ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.storageVolumes")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.additionalVolumes"),"."),(0,i.kt)("p",null,"This section shows how to configure PV using ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.storageVolumes")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.additionalVolumes"),"."),(0,i.kt)("h4",{id:"configure-using-spectidbstoragevolumes"},"Configure using ",(0,i.kt)("inlineCode",{parentName:"h4"},"spec.tidb.storageVolumes")),(0,i.kt)("p",null,"Configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"TidbCluster")," CR as the following example. In the example, TiDB Operator uses the ",(0,i.kt)("inlineCode",{parentName:"p"},"${volumeName}")," PV to store slow logs. The log file path is ",(0,i.kt)("inlineCode",{parentName:"p"},"${mountPath}/${volumeName}"),"."),(0,i.kt)("p",null,"For how to configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.storageVolumes")," field, refer to ",(0,i.kt)("a",{parentName:"p",href:"#multiple-disks-mounting"},"Multiple disks mounting"),"."),(0,i.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"You need to configure ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolumes")," before creating the cluster. After the cluster is created, adding or removing ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolumes")," is no longer supported. For the ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolumes")," already configured, except for increasing ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolume.storageSize"),", other modifications are not supported. To increase ",(0,i.kt)("inlineCode",{parentName:"p"},"storageVolume.storageSize"),", you need to make sure that the corresponding StorageClass supports ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/"},"dynamic expansion"),"."))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'  tidb:\n    ...\n    separateSlowLog: true  # can be ignored\n    slowLogVolumeName: ${volumeName}\n    storageVolumes:\n      # name must be consistent with slowLogVolumeName\n      - name: ${volumeName}\n        storageClassName: ${storageClass}\n        storageSize: "1Gi"\n        mountPath: ${mountPath}\n')),(0,i.kt)("h4",{id:"configure-using-spectidbadditionalvolumes"},"Configure using ",(0,i.kt)("inlineCode",{parentName:"h4"},"spec.tidb.additionalVolumes")),(0,i.kt)("p",null,"In the following example, NFS is used as the storage, and TiDB Operator uses the ",(0,i.kt)("inlineCode",{parentName:"p"},"${volumeName}")," PV to store slow logs. The log file path is ",(0,i.kt)("inlineCode",{parentName:"p"},"${mountPath}/${volumeName}"),"."),(0,i.kt)("p",null,"For the supported PV types, refer to ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes"},"Persistent Volumes"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"  tidb:\n    ...\n    separateSlowLog: true  # can be ignored\n    slowLogVolumeName: ${volumeName}\n    additionalVolumes:\n    # name must be consistent with slowLogVolumeName\n    - name: ${volumeName}\n      nfs:\n        server: 192.168.0.2\n        path: /nfs\n    additionalVolumeMounts:\n    # name must be consistent with slowLogVolumeName\n    - name: ${volumeName}\n      mountPath: ${mountPath}\n")),(0,i.kt)("h3",{id:"configure-tidb-service"},"Configure TiDB service"),(0,i.kt)("p",null,"You need to configure ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.service")," so that TiDB Operator creates a service for TiDB. You can configure Service with different types according to the scenarios, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"ClusterIP"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"NodePort"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"LoadBalancer"),", etc."),(0,i.kt)("h4",{id:"clusterip"},"ClusterIP"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"ClusterIP")," exposes services through the internal IP of the cluster. When selecting this type of service, you can only access it within the cluster using ClusterIP or the Service domain name (",(0,i.kt)("inlineCode",{parentName:"p"},"${cluster_name}-tidb.${namespace}"),")."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  tidb:\n    service:\n      type: ClusterIP\n")),(0,i.kt)("h4",{id:"nodeport"},"NodePort"),(0,i.kt)("p",null,"If there is no LoadBalancer, you can choose to expose the service through NodePort. NodePort exposes services through the node's IP and static port. You can access a NodePort service from outside of the cluster by requesting ",(0,i.kt)("inlineCode",{parentName:"p"},"NodeIP + NodePort"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  tidb:\n    service:\n      type: NodePort\n      # externalTrafficPolicy: Local\n")),(0,i.kt)("p",null,"NodePort has two modes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"externalTrafficPolicy=Cluster"),": All machines in the cluster allocate a NodePort port to TiDB, which is the default value."),(0,i.kt)("p",{parentName:"li"},"  When using the ",(0,i.kt)("inlineCode",{parentName:"p"},"Cluster")," mode, you can access the TiDB service through the IP and NodePort of any machine. If there is no TiDB Pod on the machine, the corresponding request will be forwarded to the machine with TiDB Pod."),(0,i.kt)("blockquote",{parentName:"li"},(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Note:")),(0,i.kt)("p",{parentName:"blockquote"},"In this mode, the request source IP obtained by the TiDB service is the host IP, not the real client source IP, so access control based on the client source IP is not available in this mode."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"externalTrafficPolicy=Local"),": Only the machine that TiDB is running on allocates a NodePort port to access the local TiDB instance."))),(0,i.kt)("h4",{id:"loadbalancer"},"LoadBalancer"),(0,i.kt)("p",null,"If the TiDB cluster runs in an environment with LoadBalancer, such as on GCP or AWS, it is recommended to use the LoadBalancer feature of these cloud platforms by setting ",(0,i.kt)("inlineCode",{parentName:"p"},"tidb.service.type=LoadBalancer"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'spec:\n  ...\n  tidb:\n    service:\n      annotations:\n        cloud.google.com/load-balancer-type: "Internal"\n      externalTrafficPolicy: Local\n      type: LoadBalancer\n')),(0,i.kt)("p",null,"See ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/services-networking/service/"},"Kubernetes Service Documentation")," to know more about the features of Service and what LoadBalancer in the cloud platform supports."),(0,i.kt)("h2",{id:"configure-high-availability"},"Configure high availability"),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"TiDB Operator provides a custom scheduler that guarantees TiDB service can tolerate host-level failures through the specified scheduling algorithm. Currently, the TiDB cluster uses this scheduler as the default scheduler, which is configured through the item ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.schedulerName"),". This section focuses on configuring a TiDB cluster to tolerate failures at other levels such as rack, zone, or region. This section is optional."))),(0,i.kt)("p",null,"TiDB is a distributed database and its high availability must ensure that when any physical topology node fails, not only the service is unaffected, but also the data is complete and available. The two configurations of high availability are described separately as follows."),(0,i.kt)("h3",{id:"high-availability-of-tidb-service"},"High availability of TiDB service"),(0,i.kt)("h4",{id:"use-affinity-to-schedule-pods"},"Use affinity to schedule pods"),(0,i.kt)("p",null,"By configuring ",(0,i.kt)("inlineCode",{parentName:"p"},"PodAntiAffinity"),", you can avoid the situation in which different instances of the same component are deployed on the same physical topology node. In this way, disaster recovery (high availability) is achieved. For the user guide of Affinity, see ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity"},"Affinity & AntiAffinity"),"."),(0,i.kt)("p",null,"The following is an example of a typical service high availability setup:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},'affinity:\n podAntiAffinity:\n   preferredDuringSchedulingIgnoredDuringExecution:\n   # this term works when the nodes have the label named region\n   - weight: 10\n     podAffinityTerm:\n       labelSelector:\n         matchLabels:\n           app.kubernetes.io/instance: ${cluster_name}\n           app.kubernetes.io/component: "pd"\n       topologyKey: "region"\n       namespaces:\n       - ${namespace}\n   # this term works when the nodes have the label named zone\n   - weight: 20\n     podAffinityTerm:\n       labelSelector:\n         matchLabels:\n           app.kubernetes.io/instance: ${cluster_name}\n           app.kubernetes.io/component: "pd"\n       topologyKey: "zone"\n       namespaces:\n       - ${namespace}\n   # this term works when the nodes have the label named rack\n   - weight: 40\n     podAffinityTerm:\n       labelSelector:\n         matchLabels:\n           app.kubernetes.io/instance: ${cluster_name}\n           app.kubernetes.io/component: "pd"\n       topologyKey: "rack"\n       namespaces:\n       - ${namespace}\n   # this term works when the nodes have the label named kubernetes.io/hostname\n   - weight: 80\n     podAffinityTerm:\n       labelSelector:\n         matchLabels:\n           app.kubernetes.io/instance: ${cluster_name}\n           app.kubernetes.io/component: "pd"\n       topologyKey: "kubernetes.io/hostname"\n       namespaces:\n       - ${namespace}\n')),(0,i.kt)("h4",{id:"use-topologyspreadconstraints-to-make-pods-evenly-spread"},"Use topologySpreadConstraints to make pods evenly spread"),(0,i.kt)("p",null,"By configuring ",(0,i.kt)("inlineCode",{parentName:"p"},"topologySpreadConstraints"),", you can make pods evenly spread in different topologies. For instructions about configuring ",(0,i.kt)("inlineCode",{parentName:"p"},"topologySpreadConstraints"),", see ",(0,i.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/"},"Pod Topology Spread Constraints"),"."),(0,i.kt)("p",null,"To use ",(0,i.kt)("inlineCode",{parentName:"p"},"topologySpreadConstraints"),", you must meet the following conditions:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Your Kubernetes cluster uses ",(0,i.kt)("inlineCode",{parentName:"li"},"default-scheduler")," instead of ",(0,i.kt)("inlineCode",{parentName:"li"},"tidb-scheduler"),". For details, refer to ",(0,i.kt)("a",{parentName:"li",href:"/docusaurus-operator/tidb-scheduler#tidb-scheduler-and-default-scheduler"},"tidb-scheduler and default-scheduler"),"."),(0,i.kt)("li",{parentName:"ul"},"Your Kubernetes cluster enables the ",(0,i.kt)("inlineCode",{parentName:"li"},"EvenPodsSpread")," feature gate. If the Kubernetes version in use is earlier than v1.16 or if the ",(0,i.kt)("inlineCode",{parentName:"li"},"EvenPodsSpread")," feature gate is disabled, the configuration of ",(0,i.kt)("inlineCode",{parentName:"li"},"topologySpreadConstraints")," does not take effect.")),(0,i.kt)("p",null,"You can either configure ",(0,i.kt)("inlineCode",{parentName:"p"},"topologySpreadConstraints")," at a cluster level (",(0,i.kt)("inlineCode",{parentName:"p"},"spec.topologySpreadConstraints"),") for all components or at a component level (such as ",(0,i.kt)("inlineCode",{parentName:"p"},"spec.tidb.topologySpreadConstraints"),") for specific components."),(0,i.kt)("p",null,"The following is an example configuration:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"topologySpreadConstraints:\n- topologyKey: kubernetes.io/hostname\n- topologyKey: topology.kubernetes.io/zone\n")),(0,i.kt)("p",null,"The example configuration can make pods of the same component evenly spread on different zones and nodes."),(0,i.kt)("p",null,"Currently, ",(0,i.kt)("inlineCode",{parentName:"p"},"topologySpreadConstraints")," only supports the configuration of the ",(0,i.kt)("inlineCode",{parentName:"p"},"topologyKey")," field. In the pod spec, the above example configuration will be automatically expanded as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"topologySpreadConstraints:\n- topologyKey: kubernetes.io/hostname\n  maxSkew: 1\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector: <object>\n- topologyKey: topology.kubernetes.io/zone\n  maxSkew: 1\n  whenUnsatisfiable: DoNotSchedule\n  labelSelector: <object>\n")),(0,i.kt)("h3",{id:"high-availability-of-data"},"High availability of data"),(0,i.kt)("p",null,"Before configuring the high availability of data, read ",(0,i.kt)("a",{parentName:"p",href:"https://pingcap.com/docs/stable/location-awareness/"},"Information Configuration of the Cluster Typology")," which describes how high availability of TiDB cluster is implemented."),(0,i.kt)("p",null,"To add the data high availability feature in Kubernetes:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Set the label collection of topological location for PD"),(0,i.kt)("p",{parentName:"li"},"Replace the ",(0,i.kt)("inlineCode",{parentName:"p"},"location-labels")," information in the ",(0,i.kt)("inlineCode",{parentName:"p"},"pd.config")," with the label collection that describes the topological location on the nodes in the Kubernetes cluster."),(0,i.kt)("blockquote",{parentName:"li"},(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Note:")),(0,i.kt)("ul",{parentName:"blockquote"},(0,i.kt)("li",{parentName:"ul"},"For PD versions < v3.0.9, the ",(0,i.kt)("inlineCode",{parentName:"li"},"/")," in the label name is not supported."),(0,i.kt)("li",{parentName:"ul"},"If you configure ",(0,i.kt)("inlineCode",{parentName:"li"},"host")," in the ",(0,i.kt)("inlineCode",{parentName:"li"},"location-labels"),", TiDB Operator will get the value from the ",(0,i.kt)("inlineCode",{parentName:"li"},"kubernetes.io/hostname")," in the node label.")))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Set the topological information of the Node where the TiKV node is located."),(0,i.kt)("p",{parentName:"li"},"TiDB Operator automatically obtains the topological information of the Node for TiKV and calls the PD interface to set this information as the information of TiKV's store labels. Based on this topological information, the TiDB cluster schedules the replicas of the data."),(0,i.kt)("p",{parentName:"li"},"If the Node of the current Kubernetes cluster does not have a label indicating the topological location, or if the existing label name of topology contains ",(0,i.kt)("inlineCode",{parentName:"p"},"/"),", you can manually add a label to the Node by running the following command:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl label node ${node_name} region=${region_name} zone=${zone_name} rack=${rack_name} kubernetes.io/hostname=${host_name}\n")),(0,i.kt)("p",{parentName:"li"},"In the command above, ",(0,i.kt)("inlineCode",{parentName:"p"},"region"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"zone"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"rack"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"kubernetes.io/hostname")," are just examples. The name and number of the label to be added can be arbitrarily defined, as long as it conforms to the specification and is consistent with the labels set by ",(0,i.kt)("inlineCode",{parentName:"p"},"location-labels")," in ",(0,i.kt)("inlineCode",{parentName:"p"},"pd.config"),"."))))}m.isMDXComponent=!0}}]);