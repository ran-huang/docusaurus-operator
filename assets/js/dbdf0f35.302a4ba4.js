"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[5384],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return u}});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(a),u=i,h=m["".concat(s,".").concat(u)]||m[u]||d[u]||o;return a?n.createElement(h,r(r({ref:t},c),{},{components:a})):n.createElement(h,r({ref:t},c))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<o;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},9306:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},assets:function(){return c},toc:function(){return d},default:function(){return u}});var n=a(7462),i=a(3366),o=(a(7294),a(3905)),r=["components"],l={title:"Deploy TiDB on AWS EKS",summary:"Learn how to deploy a TiDB cluster on AWS Elastic Kubernetes Service (EKS)."},s="Deploy TiDB on AWS EKS",p={unversionedId:"deploy-on-aws-eks",id:"deploy-on-aws-eks",title:"Deploy TiDB on AWS EKS",description:"This document describes how to deploy a TiDB cluster on AWS Elastic Kubernetes Service (EKS).",source:"@site/docs/deploy-on-aws-eks.md",sourceDirName:".",slug:"/deploy-on-aws-eks",permalink:"/deploy-on-aws-eks",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/deploy-on-aws-eks.md",tags:[],version:"current",frontMatter:{title:"Deploy TiDB on AWS EKS",summary:"Learn how to deploy a TiDB cluster on AWS Elastic Kubernetes Service (EKS)."},sidebar:"mySidebar",previous:{title:"Access the TiDB Cluster in Kubernetes",permalink:"/access-tidb"},next:{title:"Deploy TiDB on GCP GKE",permalink:"/deploy-on-gcp-gke"}},c={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Recommended instance types and storage",id:"recommended-instance-types-and-storage",level:2},{value:"Create an EKS cluster and a node pool",id:"create-an-eks-cluster-and-a-node-pool",level:2},{value:"Configure StorageClass",id:"configure-storageclass",level:2},{value:"Configure <code>gp2</code>",id:"configure-gp2",level:3},{value:"Configure <code>gp3</code> (recommended) or other EBS storage types",id:"configure-gp3-recommended-or-other-ebs-storage-types",level:3},{value:"Configure local storage",id:"configure-local-storage",level:3},{value:"Deploy TiDB Operator",id:"deploy-tidb-operator",level:2},{value:"Deploy a TiDB cluster and the monitoring component",id:"deploy-a-tidb-cluster-and-the-monitoring-component",level:2},{value:"Create namespace",id:"create-namespace",level:3},{value:"Deploy",id:"deploy",level:3},{value:"View the cluster status",id:"view-the-cluster-status",level:3},{value:"Access the database",id:"access-the-database",level:2},{value:"Prepare a bastion host",id:"prepare-a-bastion-host",level:3},{value:"Install the MySQL client and connect",id:"install-the-mysql-client-and-connect",level:3},{value:"Access the Grafana monitoring dashboard",id:"access-the-grafana-monitoring-dashboard",level:2},{value:"Access the TiDB Dashboard",id:"access-the-tidb-dashboard",level:2},{value:"Upgrade",id:"upgrade",level:2},{value:"Scale out",id:"scale-out",level:2},{value:"Scale out EKS node group",id:"scale-out-eks-node-group",level:3},{value:"Scale out TiDB components",id:"scale-out-tidb-components",level:3},{value:"Deploy TiFlash/TiCDC",id:"deploy-tiflashticdc",level:2},{value:"Add node groups",id:"add-node-groups",level:3},{value:"Configure and deploy",id:"configure-and-deploy",level:3},{value:"Deploy TiDB Enterprise Edition",id:"deploy-tidb-enterprise-edition",level:2}],m={toc:d};function u(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"deploy-tidb-on-aws-eks"},"Deploy TiDB on AWS EKS"),(0,o.kt)("p",null,"This document describes how to deploy a TiDB cluster on AWS Elastic Kubernetes Service (EKS)."),(0,o.kt)("p",null,"To deploy TiDB Operator and the TiDB cluster in a self-managed Kubernetes environment, refer to ",(0,o.kt)("a",{parentName:"p",href:"/deploy-tidb-operator"},"Deploy TiDB Operator")," and ",(0,o.kt)("a",{parentName:"p",href:"/deploy-on-general-kubernetes"},"Deploy TiDB in General Kubernetes"),"."),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("p",null,"Before deploying a TiDB cluster on AWS EKS, make sure the following requirements are satisfied:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Install ",(0,o.kt)("a",{parentName:"p",href:"https://helm.sh/docs/intro/install/"},"Helm 3"),": used for deploying TiDB Operator.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Complete all operations in ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html"},"Getting started with eksctl"),"."),(0,o.kt)("p",{parentName:"li"},"  This guide includes the following contents:"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Install and configure ",(0,o.kt)("inlineCode",{parentName:"li"},"awscli"),"."),(0,o.kt)("li",{parentName:"ul"},"Install and configure ",(0,o.kt)("inlineCode",{parentName:"li"},"eksctl")," used for creating Kubernetes clusters."),(0,o.kt)("li",{parentName:"ul"},"Install ",(0,o.kt)("inlineCode",{parentName:"li"},"kubectl"),".")))),(0,o.kt)("p",null,"To verify whether AWS CLI is configured correctly, run the ",(0,o.kt)("inlineCode",{parentName:"p"},"aws configure list")," command. If the output shows the values for ",(0,o.kt)("inlineCode",{parentName:"p"},"access_key")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"secret_key"),", AWS CLI is configured correctly. Otherwise, you need to re-configure AWS CLI."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"The operations described in this document require at least the ",(0,o.kt)("a",{parentName:"p",href:"https://eksctl.io/usage/minimum-iam-policies/"},"minimum privileges needed by ",(0,o.kt)("inlineCode",{parentName:"a"},"eksctl"))," and the ",(0,o.kt)("a",{parentName:"p",href:"https://aws-quickstart.github.io/quickstart-linux-bastion/#_aws_account"},"service privileges needed to create a Linux bastion host"),"."))),(0,o.kt)("h2",{id:"recommended-instance-types-and-storage"},"Recommended instance types and storage"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Instance types: to gain better performance, the following is recommended:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"PD nodes: ",(0,o.kt)("inlineCode",{parentName:"li"},"c5.xlarge")),(0,o.kt)("li",{parentName:"ul"},"TiDB nodes: ",(0,o.kt)("inlineCode",{parentName:"li"},"c5.2xlarge")),(0,o.kt)("li",{parentName:"ul"},"TiKV or TiFlash nodes: ",(0,o.kt)("inlineCode",{parentName:"li"},"r5b.2xlarge")))),(0,o.kt)("li",{parentName:"ul"},"Storage: Because AWS supports the ",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#gp3-ebs-volume-type"},"EBS ",(0,o.kt)("inlineCode",{parentName:"a"},"gp3"))," volume type, it is recommended to use EBS ",(0,o.kt)("inlineCode",{parentName:"li"},"gp3"),". For ",(0,o.kt)("inlineCode",{parentName:"li"},"gp3")," provisioning, the following is recommended:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"TiKV: 400 MiB/s, 4000 IOPS"),(0,o.kt)("li",{parentName:"ul"},"TiFlash: 625 MiB/s, 6000 IOPS"))),(0,o.kt)("li",{parentName:"ul"},"AMI type: Amazon Linux 2")),(0,o.kt)("h2",{id:"create-an-eks-cluster-and-a-node-pool"},"Create an EKS cluster and a node pool"),(0,o.kt)("p",null,"According to AWS ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/"},"Official Blog")," recommendation and EKS ",(0,o.kt)("a",{parentName:"p",href:"https://aws.github.io/aws-eks-best-practices/reliability/docs/dataplane/#ensure-capacity-in-each-az-when-using-ebs-volumes"},"Best Practice Document"),", since most of the TiDB cluster components use EBS volumes as storage, it is recommended to create a node pool in each availability zone (at least 3 in total) for each component when creating an EKS."),(0,o.kt)("p",null,"Save the following configuration as the ",(0,o.kt)("inlineCode",{parentName:"p"},"cluster.yaml")," file. Replace ",(0,o.kt)("inlineCode",{parentName:"p"},"${clusterName}")," with your desired cluster name. The cluster and node group names should match the regular expression ",(0,o.kt)("inlineCode",{parentName:"p"},"[a-zA-Z][-a-zA-Z0-9]*"),", so avoid names that contain ",(0,o.kt)("inlineCode",{parentName:"p"},"_"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\n  name: ${clusterName}\n  region: ap-northeast-1\n\nnodeGroups:\n  - name: admin\n    desiredCapacity: 1\n    privateNetworking: true\n    labels:\n      dedicated: admin\n\n  - name: tidb-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    instanceType: c5.2xlarge\n    labels:\n      dedicated: tidb\n    taints:\n      dedicated: tidb:NoSchedule\n  - name: tidb-1d\n    desiredCapacity: 0\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1d"]\n    instanceType: c5.2xlarge\n    labels:\n      dedicated: tidb\n    taints:\n      dedicated: tidb:NoSchedule\n  - name: tidb-1c\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1c"]\n    instanceType: c5.2xlarge\n    labels:\n      dedicated: tidb\n    taints:\n      dedicated: tidb:NoSchedule\n\n  - name: pd-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    instanceType: c5.xlarge\n    labels:\n      dedicated: pd\n    taints:\n      dedicated: pd:NoSchedule\n  - name: pd-1d\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1d"]\n    instanceType: c5.xlarge\n    labels:\n      dedicated: pd\n    taints:\n      dedicated: pd:NoSchedule\n  - name: pd-1c\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1c"]\n    instanceType: c5.xlarge\n    labels:\n      dedicated: pd\n    taints:\n      dedicated: pd:NoSchedule\n\n  - name: tikv-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    instanceType: r5b.2xlarge\n    labels:\n      dedicated: tikv\n    taints:\n      dedicated: tikv:NoSchedule\n  - name: tikv-1d\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1d"]\n    instanceType: r5b.2xlarge\n    labels:\n      dedicated: tikv\n    taints:\n      dedicated: tikv:NoSchedule\n  - name: tikv-1c\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1c"]\n    instanceType: r5b.2xlarge\n    labels:\n      dedicated: tikv\n    taints:\n      dedicated: tikv:NoSchedule\n')),(0,o.kt)("p",null,"By default, only two TiDB nodes are required, so you can set the ",(0,o.kt)("inlineCode",{parentName:"p"},"desiredCapacity")," of the ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-1d")," node group to ",(0,o.kt)("inlineCode",{parentName:"p"},"0"),". You can scale out this node group any time if necessary."),(0,o.kt)("p",null,"Execute the following command to create the cluster:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"eksctl create cluster -f cluster.yaml\n")),(0,o.kt)("p",null,"After executing the command above, you need to wait until the EKS cluster is successfully created and the node group is created and added in the EKS cluster. This process might take 5 to 20 minutes. For more cluster configuration, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://eksctl.io/usage/creating-and-managing-clusters/#using-config-files"},(0,o.kt)("inlineCode",{parentName:"a"},"eksctl")," documentation"),"."),(0,o.kt)("div",{className:"admonition admonition-danger alert alert--danger"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"Warning")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"If the Regional Auto Scaling Group (ASG) is used:"),(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#instance-protection-instance"},"Enable the instance scale-in protection")," for all the EC2s that have been started. The instance scale-in protection for the ASG is not required."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#custom-termination-policy"},"Set termination policy")," to ",(0,o.kt)("inlineCode",{parentName:"li"},"NewestInstance")," for the ASG.")))),(0,o.kt)("h2",{id:"configure-storageclass"},"Configure StorageClass"),(0,o.kt)("p",null,"This section describes how to configure the storage class for different storage types. These storage types are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The default ",(0,o.kt)("inlineCode",{parentName:"li"},"gp2")," storage type after creating the EKS cluster."),(0,o.kt)("li",{parentName:"ul"},"The ",(0,o.kt)("inlineCode",{parentName:"li"},"gp3")," storage type (recommended) or other EBS storage types."),(0,o.kt)("li",{parentName:"ul"},"The local storage used for testing bare-metal performance.")),(0,o.kt)("h3",{id:"configure-gp2"},"Configure ",(0,o.kt)("inlineCode",{parentName:"h3"},"gp2")),(0,o.kt)("p",null,"After you create an EKS cluster, the default StorageClass is ",(0,o.kt)("inlineCode",{parentName:"p"},"gp2"),". To improve I/O write performance, it is recommended to configure ",(0,o.kt)("inlineCode",{parentName:"p"},"nodelalloc")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"noatime")," in the ",(0,o.kt)("inlineCode",{parentName:"p"},"mountOptions")," field of the ",(0,o.kt)("inlineCode",{parentName:"p"},"StorageClass")," resource."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"kind: StorageClass\napiVersion: storage.k8s.io/v1\n# ...\nmountOptions:\n- nodelalloc,noatime\n")),(0,o.kt)("p",null,"For more information on the mount options, see ",(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/check-before-deployment#mount-the-data-disk-ext4-filesystem-with-options-on-the-target-machines-that-deploy-tikv"},"TiDB Environment and System Configuration Check"),"."),(0,o.kt)("h3",{id:"configure-gp3-recommended-or-other-ebs-storage-types"},"Configure ",(0,o.kt)("inlineCode",{parentName:"h3"},"gp3")," (recommended) or other EBS storage types"),(0,o.kt)("p",null,"If you do not want to use the default ",(0,o.kt)("inlineCode",{parentName:"p"},"gp2")," storage type, you can create StorageClass for other storage types. For example, you can use the ",(0,o.kt)("inlineCode",{parentName:"p"},"gp3")," (recommended) or ",(0,o.kt)("inlineCode",{parentName:"p"},"io1")," storage type."),(0,o.kt)("p",null,"The following example shows how to create and configure a StorageClass for the ",(0,o.kt)("inlineCode",{parentName:"p"},"gp3")," storage type:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Deploy the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html"},"AWS EBS Container Storage Interface (CSI) driver")," on the EKS cluster. If you are using a storage type other than ",(0,o.kt)("inlineCode",{parentName:"p"},"gp3"),", skip this step.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Set ebs-csi-node ",(0,o.kt)("inlineCode",{parentName:"p"},"toleration"),"."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl patch -n kube-system ds ebs-csi-node -p \'{"spec":{"template":{"spec":{"tolerations":[{"operator":"Exists"}]}}}}\'\n')),(0,o.kt)("p",{parentName:"li"},"Expected output:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre"},"daemonset.apps/ebs-csi-node patched\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a ",(0,o.kt)("inlineCode",{parentName:"p"},"StorageClass")," resource. In the resource definition, specify your desired storage type in the ",(0,o.kt)("inlineCode",{parentName:"p"},"parameters.type")," field."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: gp3\nprovisioner: ebs.csi.aws.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  type: gp3\n  fsType: ext4\n  iops: "4000"\n  throughput: "400"\nmountOptions:\n- nodelalloc,noatime\n'))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"In the TidbCluster YAML file, configure ",(0,o.kt)("inlineCode",{parentName:"p"},"gp3")," in the ",(0,o.kt)("inlineCode",{parentName:"p"},"storageClassName")," field. For example:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  tikv:\n    ...\n    storageClassName: gp3\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"To improve I/O write performance, it is recommended to configure ",(0,o.kt)("inlineCode",{parentName:"p"},"nodelalloc")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"noatime")," in the ",(0,o.kt)("inlineCode",{parentName:"p"},"mountOptions")," field of the ",(0,o.kt)("inlineCode",{parentName:"p"},"StorageClass")," resource."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"kind: StorageClass\napiVersion: storage.k8s.io/v1\n# ...\nmountOptions:\n- nodelalloc,noatime\n")),(0,o.kt)("p",{parentName:"li"},"For more information on the mount options, see ",(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/check-before-deployment#mount-the-data-disk-ext4-filesystem-with-options-on-the-target-machines-that-deploy-tikv"},"TiDB Environment and System Configuration Check"),"."))),(0,o.kt)("p",null,"For more information on the EBS storage types and configuration, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"},"Amazon EBS volume types")," and ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/storage-classes/"},"Storage Classes"),"."),(0,o.kt)("h3",{id:"configure-local-storage"},"Configure local storage"),(0,o.kt)("p",null,"Local storage is used for testing bare-metal performance. For higher IOPS and lower latency, you can choose ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssd-instance-store.html"},"NVMe SSD volumes")," offered by some AWS instances for the TiKV node pool. However, for the production environment, use AWS EBS as your storage type."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},"You cannot dynamically change StorageClass for a running TiDB cluster. For testing purposes, create a new TiDB cluster with the desired StorageClass."),(0,o.kt)("li",{parentName:"ul"},"EKS upgrade or other reasons might cause node reconstruction. In such cases, ",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-lifetime"},"data in the local storage might be lost"),". To avoid data loss, you need to back up TiKV data before node reconstruction."),(0,o.kt)("li",{parentName:"ul"},"To avoid data loss from node reconstruction, you can refer to ",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html"},"AWS documentation")," and disable the ",(0,o.kt)("inlineCode",{parentName:"li"},"ReplaceUnhealthy")," feature of the TiKV node group.")))),(0,o.kt)("p",null,"For instance types that provide NVMe SSD volumes, check out ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/ec2/instance-types/"},"Amazon EC2 Instance Types"),"."),(0,o.kt)("p",null,"The following ",(0,o.kt)("inlineCode",{parentName:"p"},"c5d.4xlarge")," example shows how to configure StorageClass for the local storage:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a node group with local storage for TiKV."),(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"In the ",(0,o.kt)("inlineCode",{parentName:"p"},"eksctl")," configuration file, modify the instance type of the TiKV node group to ",(0,o.kt)("inlineCode",{parentName:"p"},"c5d.4xlarge"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'  - name: tikv-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    instanceType: c5d.4xlarge\n    labels:\n      dedicated: tikv\n    taints:\n      dedicated: tikv:NoSchedule\n    ...\n'))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create a node group with local storage:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"eksctl create nodegroups -f cluster.yaml\n")))),(0,o.kt)("p",{parentName:"li"},"If the TiKV node group already exists, to avoid name conflict, you can take either of the following actions:"),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Delete the old group and create a new one."),(0,o.kt)("li",{parentName:"ul"},"Change the group name."))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Deploy local volume provisioner."),(0,o.kt)("ol",{parentName:"li"},(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"To conveniently discover and manage local storage volumes, install ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner"},"local-volume-provisioner"),".")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("a",{parentName:"p",href:"https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner/blob/master/docs/operations.md#use-a-whole-disk-as-a-filesystem-pv"},"Mount the local storage")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"/mnt/ssd")," directory.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"According to the mounting configuration, modify the ",(0,o.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/eks/local-volume-provisioner.yaml"},"local-volume-provisioner.yaml")," file.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Deploy and create a ",(0,o.kt)("inlineCode",{parentName:"p"},"local-storage")," storage class using the modified ",(0,o.kt)("inlineCode",{parentName:"p"},"local-volume-provisioner.yaml")," file."),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f <local-volume-provisioner.yaml>\n"))))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Use the local storage."),(0,o.kt)("p",{parentName:"li"},"After you complete the previous step, local-volume-provisioner can discover all the local NVMe SSD volumes in the cluster."))),(0,o.kt)("p",null,"After local-volume-provisioner discovers the local volumes, when you ",(0,o.kt)("a",{parentName:"p",href:"#deploy-a-tidb-cluster-and-the-monitoring-component"},"Deploy a TiDB cluster and the monitoring component"),", you need to add the ",(0,o.kt)("inlineCode",{parentName:"p"},"tikv.storageClassName")," field to ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml")," and set the field value to ",(0,o.kt)("inlineCode",{parentName:"p"},"local-storage"),"."),(0,o.kt)("h2",{id:"deploy-tidb-operator"},"Deploy TiDB Operator"),(0,o.kt)("p",null,"To deploy TiDB Operator in the EKS cluster, refer to the ",(0,o.kt)("a",{parentName:"p",href:"/get-started#step-2-deploy-tidb-operator"},(0,o.kt)("em",{parentName:"a"},"Deploy TiDB Operator")," section")," in Getting Started."),(0,o.kt)("h2",{id:"deploy-a-tidb-cluster-and-the-monitoring-component"},"Deploy a TiDB cluster and the monitoring component"),(0,o.kt)("p",null,"This section describes how to deploy a TiDB cluster and its monitoring component in AWS EKS."),(0,o.kt)("h3",{id:"create-namespace"},"Create namespace"),(0,o.kt)("p",null,"To create a namespace to deploy the TiDB cluster, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl create namespace tidb-cluster\n")),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"A ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"},(0,o.kt)("inlineCode",{parentName:"a"},"namespace"))," is a virtual cluster backed by the same physical cluster. This document takes ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster")," as an example. If you want to use another namespace, modify the corresponding arguments of ",(0,o.kt)("inlineCode",{parentName:"p"},"-n")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"--namespace"),"."))),(0,o.kt)("h3",{id:"deploy"},"Deploy"),(0,o.kt)("p",null,"First, download the sample ",(0,o.kt)("inlineCode",{parentName:"p"},"TidbCluster")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"TidbMonitor")," configuration files:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"curl -O https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/aws/tidb-cluster.yaml && \\\ncurl -O https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/aws/tidb-monitor.yaml\n")),(0,o.kt)("p",null,"Refer to ",(0,o.kt)("a",{parentName:"p",href:"/configure-a-tidb-cluster"},"configure the TiDB cluster")," to further customize and configure the CR before applying."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"By default, the configuration in ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml"),' sets up the LoadBalancer for TiDB with the "internal" scheme. This means that the LoadBalancer is only accessible within the VPC, not externally. To access TiDB over the MySQL protocol, you need to use a bastion host or use ',(0,o.kt)("inlineCode",{parentName:"p"},"kubectl port-forward"),'. If you want to expose TiDB over the internet and if you are aware of the risks of doing this, you can change the scheme for the LoadBalancer from "internal" to "internet-facing" in the ',(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml")," file."))),(0,o.kt)("p",null,"To deploy the ",(0,o.kt)("inlineCode",{parentName:"p"},"TidbCluster")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"TidbMonitor")," CR in the EKS cluster, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f tidb-cluster.yaml -n tidb-cluster && \\\nkubectl apply -f tidb-monitor.yaml -n tidb-cluster\n")),(0,o.kt)("p",null,"After the YAML file above is applied to the Kubernetes cluster, TiDB Operator creates the desired TiDB cluster and its monitoring component according to the YAML file."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"If you need to deploy a TiDB cluster on ARM64 machines, refer to ",(0,o.kt)("a",{parentName:"p",href:"/deploy-cluster-on-arm64"},"Deploy a TiDB Cluster on ARM64 Machines"),"."))),(0,o.kt)("h3",{id:"view-the-cluster-status"},"View the cluster status"),(0,o.kt)("p",null,"To view the status of the starting TiDB cluster, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get pods -n tidb-cluster\n")),(0,o.kt)("p",null,"When all the Pods are in the ",(0,o.kt)("inlineCode",{parentName:"p"},"Running")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"Ready")," state, the TiDB cluster is successfully started. For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"NAME                              READY   STATUS    RESTARTS   AGE\ntidb-discovery-5cb8474d89-n8cxk   1/1     Running   0          47h\ntidb-monitor-6fbcc68669-dsjlc     3/3     Running   0          47h\ntidb-pd-0                         1/1     Running   0          47h\ntidb-pd-1                         1/1     Running   0          46h\ntidb-pd-2                         1/1     Running   0          46h\ntidb-tidb-0                       2/2     Running   0          47h\ntidb-tidb-1                       2/2     Running   0          46h\ntidb-tikv-0                       1/1     Running   0          47h\ntidb-tikv-1                       1/1     Running   0          47h\ntidb-tikv-2                       1/1     Running   0          47h\n")),(0,o.kt)("h2",{id:"access-the-database"},"Access the database"),(0,o.kt)("p",null,"After you have deployed a TiDB cluster, you can access the TiDB database to test or develop your application."),(0,o.kt)("h3",{id:"prepare-a-bastion-host"},"Prepare a bastion host"),(0,o.kt)("p",null,"The LoadBalancer created for your TiDB cluster is an intranet LoadBalancer. You can create a ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/quickstart/architecture/linux-bastion/"},"bastion host")," in the cluster VPC to access the database. To create a bastion host on AWS console, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/quickstart/architecture/linux-bastion/"},"AWS documentation"),"."),(0,o.kt)("p",null,"Select the cluster's VPC and Subnet, and verify whether the cluster name is correct in the dropdown box. You can view the cluster's VPC and Subnet by running the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"eksctl get cluster -n ${clusterName}\n")),(0,o.kt)("p",null,"Allow the bastion host to access the Internet. Select the correct key pair so that you can log in to the host via SSH."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"In addition to the bastion host, you can also connect an existing host to the cluster VPC by ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html"},"VPC Peering"),". If the EKS cluster is created in an existing VPC, you can use the host in the VPC."))),(0,o.kt)("h3",{id:"install-the-mysql-client-and-connect"},"Install the MySQL client and connect"),(0,o.kt)("p",null,"After the bastion host is created, you can connect to the bastion host via SSH and access the TiDB cluster via the MySQL client."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Log in to the bastion host via SSH:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"ssh [-i /path/to/your/private-key.pem] ec2-user@<bastion-public-dns-name>\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Install the MySQL client on the bastion host:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"sudo yum install mysql -y\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Connect the client to the TiDB cluster:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"mysql -h ${tidb-nlb-dnsname} -P 4000 -u root\n")),(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"${tidb-nlb-dnsname}")," is the LoadBalancer domain name of the TiDB service. You can view the domain name in the ",(0,o.kt)("inlineCode",{parentName:"p"},"EXTERNAL-IP")," field by executing ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl get svc basic-tidb -n tidb-cluster"),"."),(0,o.kt)("p",{parentName:"li"},"For example:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ mysql -h abfc623004ccb4cc3b363f3f37475af1-9774d22c27310bc1.elb.us-west-2.amazonaws.com -P 4000 -u root\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 1189\nServer version: 5.7.25-TiDB-v4.0.2 TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMySQL [(none)]> show status;\n+--------------------+--------------------------------------+\n| Variable_name      | Value                                |\n+--------------------+--------------------------------------+\n| Ssl_cipher         |                                      |\n| Ssl_cipher_list    |                                      |\n| Ssl_verify_mode    | 0                                    |\n| Ssl_version        |                                      |\n| ddl_schema_version | 22                                   |\n| server_id          | ed4ba88b-436a-424d-9087-977e897cf5ec |\n+--------------------+--------------------------------------+\n6 rows in set (0.00 sec)\n")))),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_default_authentication_plugin"},"The default authentication plugin of MySQL 8.0")," is updated from ",(0,o.kt)("inlineCode",{parentName:"li"},"mysql_native_password")," to ",(0,o.kt)("inlineCode",{parentName:"li"},"caching_sha2_password"),". Therefore, if you use MySQL client from MySQL 8.0 to access the TiDB service (cluster version < v4.0.7), and if the user account has a password, you need to explicitly specify the ",(0,o.kt)("inlineCode",{parentName:"li"},"--default-auth=mysql_native_password")," parameter."),(0,o.kt)("li",{parentName:"ul"},"By default, TiDB (starting from v4.0.2) periodically shares usage details with PingCAP to help understand how to improve the product. For details about what is shared and how to disable the sharing, see ",(0,o.kt)("a",{parentName:"li",href:"https://docs.pingcap.com/tidb/stable/telemetry"},"Telemetry"),".")))),(0,o.kt)("h2",{id:"access-the-grafana-monitoring-dashboard"},"Access the Grafana monitoring dashboard"),(0,o.kt)("p",null,"Obtain the LoadBalancer domain name of Grafana:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl -n tidb-cluster get svc basic-grafana\n")),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"$ kubectl get svc basic-grafana\nNAME            TYPE           CLUSTER-IP      EXTERNAL-IP                                                             PORT(S)          AGE\nbasic-grafana   LoadBalancer   10.100.199.42   a806cfe84c12a4831aa3313e792e3eed-1964630135.us-west-2.elb.amazonaws.com 3000:30761/TCP   121m\n")),(0,o.kt)("p",null,"In the output above, the ",(0,o.kt)("inlineCode",{parentName:"p"},"EXTERNAL-IP")," column is the LoadBalancer domain name."),(0,o.kt)("p",null,"You can access the ",(0,o.kt)("inlineCode",{parentName:"p"},"${grafana-lb}:3000")," address using your web browser to view monitoring metrics. Replace ",(0,o.kt)("inlineCode",{parentName:"p"},"${grafana-lb}")," with the LoadBalancer domain name."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"The default Grafana username and password are both ",(0,o.kt)("inlineCode",{parentName:"p"},"admin"),"."))),(0,o.kt)("h2",{id:"access-the-tidb-dashboard"},"Access the TiDB Dashboard"),(0,o.kt)("p",null,"See ",(0,o.kt)("a",{parentName:"p",href:"/access-dashboard"},"Access TiDB Dashboard")," for instructions about how to securely allow access to the TiDB Dashboard."),(0,o.kt)("h2",{id:"upgrade"},"Upgrade"),(0,o.kt)("p",null,"To upgrade the TiDB cluster, execute the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},'kubectl patch tc basic -n tidb-cluster --type merge -p \'{"spec":{"version":"${version}"}}`.\n')),(0,o.kt)("p",null,"The upgrade process does not finish immediately. You can watch the upgrade progress by executing ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl get pods -n tidb-cluster --watch"),"."),(0,o.kt)("h2",{id:"scale-out"},"Scale out"),(0,o.kt)("p",null,"Before scaling out the cluster, you need to scale out the corresponding node group so that the new instances have enough resources for operation."),(0,o.kt)("p",null,"This section describes how to scale out the EKS node group and TiDB components."),(0,o.kt)("h3",{id:"scale-out-eks-node-group"},"Scale out EKS node group"),(0,o.kt)("p",null,"When scaling out TiKV, the node groups must be scaled out evenly among the different availability zones. The following example shows how to scale out the ",(0,o.kt)("inlineCode",{parentName:"p"},"tikv-1a"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"tikv-1c"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"tikv-1d")," groups of the ",(0,o.kt)("inlineCode",{parentName:"p"},"${clusterName}")," cluster to 2 nodes:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"eksctl scale nodegroup --cluster ${clusterName} --name tikv-1a --nodes 2 --nodes-min 2 --nodes-max 2\neksctl scale nodegroup --cluster ${clusterName} --name tikv-1c --nodes 2 --nodes-min 2 --nodes-max 2\neksctl scale nodegroup --cluster ${clusterName} --name tikv-1d --nodes 2 --nodes-min 2 --nodes-max 2\n")),(0,o.kt)("p",null,"For more information on managing node groups, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://eksctl.io/usage/managing-nodegroups/"},(0,o.kt)("inlineCode",{parentName:"a"},"eksctl")," documentation"),"."),(0,o.kt)("h3",{id:"scale-out-tidb-components"},"Scale out TiDB components"),(0,o.kt)("p",null,"After scaling out the EKS node group, execute ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl edit tc basic -n tidb-cluster"),", and modify each component's ",(0,o.kt)("inlineCode",{parentName:"p"},"replicas")," to the desired number of replicas. The scaling-out process is then completed."),(0,o.kt)("h2",{id:"deploy-tiflashticdc"},"Deploy TiFlash/TiCDC"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/tiflash-overview"},"TiFlash")," is the columnar storage extension of TiKV."),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/ticdc-overview"},"TiCDC")," is a tool for replicating the incremental data of TiDB by pulling TiKV change logs."),(0,o.kt)("p",null,"The two components are ",(0,o.kt)("em",{parentName:"p"},"not required")," in the deployment. This section shows a quick start example."),(0,o.kt)("h3",{id:"add-node-groups"},"Add node groups"),(0,o.kt)("p",null,"In the configuration file of eksctl (",(0,o.kt)("inlineCode",{parentName:"p"},"cluster.yaml"),"), add the following two items to add a node group for TiFlash/TiCDC respectively. ",(0,o.kt)("inlineCode",{parentName:"p"},"desiredCapacity")," is the number of nodes you desire."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'  - name: tiflash-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    labels:\n      dedicated: tiflash\n    taints:\n      dedicated: tiflash:NoSchedule\n  - name: tiflash-1d\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1d"]\n    labels:\n      dedicated: tiflash\n    taints:\n      dedicated: tiflash:NoSchedule\n  - name: tiflash-1c\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1c"]\n    labels:\n      dedicated: tiflash\n    taints:\n      dedicated: tiflash:NoSchedule\n\n  - name: ticdc-1a\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1a"]\n    labels:\n      dedicated: ticdc\n    taints:\n      dedicated: ticdc:NoSchedule\n  - name: ticdc-1d\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1d"]\n    labels:\n      dedicated: ticdc\n    taints:\n      dedicated: ticdc:NoSchedule\n  - name: ticdc-1c\n    desiredCapacity: 1\n    privateNetworking: true\n    availabilityZones: ["ap-northeast-1c"]\n    labels:\n      dedicated: ticdc\n    taints:\n      dedicated: ticdc:NoSchedule\n')),(0,o.kt)("p",null,"Depending on the EKS cluster status, use different commands:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"If the cluster is not created, execute ",(0,o.kt)("inlineCode",{parentName:"li"},"eksctl create cluster -f cluster.yaml")," to create the cluster and node groups."),(0,o.kt)("li",{parentName:"ul"},"If the cluster is already created, execute ",(0,o.kt)("inlineCode",{parentName:"li"},"eksctl create nodegroup -f cluster.yaml")," to create the node groups. The existing node groups are ignored and will not be created again.")),(0,o.kt)("h3",{id:"configure-and-deploy"},"Configure and deploy"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"To deploy TiFlash, configure ",(0,o.kt)("inlineCode",{parentName:"p"},"spec.tiflash")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  tiflash:\n    baseImage: pingcap/tiflash\n    maxFailoverCount: 0\n    replicas: 1\n    storageClaims:\n    - resources:\n        requests:\n          storage: 100Gi\n    tolerations:\n    - effect: NoSchedule\n      key: dedicated\n      operator: Equal\n      value: tiflash\n")),(0,o.kt)("p",{parentName:"li"},"  For other parameters, refer to ",(0,o.kt)("a",{parentName:"p",href:"/configure-a-tidb-cluster"},"Configure a TiDB Cluster"),"."),(0,o.kt)("blockquote",{parentName:"li"},(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"Warning:")),(0,o.kt)("p",{parentName:"blockquote"},"TiDB Operator automatically mount PVs ",(0,o.kt)("strong",{parentName:"p"},"in the order of the configuration")," in the ",(0,o.kt)("inlineCode",{parentName:"p"},"storageClaims")," list. Therefore, if you need to add disks for TiFlash, make sure that you add the disks ",(0,o.kt)("strong",{parentName:"p"},"only to the end of the original configuration")," in the list. In addition, you must ",(0,o.kt)("strong",{parentName:"p"},"not")," alter the order of the original configuration."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"To deploy TiCDC, configure ",(0,o.kt)("inlineCode",{parentName:"p"},"spec.ticdc")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  ticdc:\n    baseImage: pingcap/ticdc\n    replicas: 1\n    tolerations:\n    - effect: NoSchedule\n      key: dedicated\n      operator: Equal\n      value: ticdc\n")),(0,o.kt)("p",{parentName:"li"},"  Modify ",(0,o.kt)("inlineCode",{parentName:"p"},"replicas")," according to your needs."))),(0,o.kt)("p",null,"Finally, execute ",(0,o.kt)("inlineCode",{parentName:"p"},"kubectl -n tidb-cluster apply -f tidb-cluster.yaml")," to update the TiDB cluster configuration."),(0,o.kt)("p",null,"For detailed CR configuration, refer to ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/pingcap/tidb-operator/blob/master/docs/api-references/docs.md"},"API references")," and ",(0,o.kt)("a",{parentName:"p",href:"/configure-a-tidb-cluster"},"Configure a TiDB Cluster"),"."),(0,o.kt)("h2",{id:"deploy-tidb-enterprise-edition"},"Deploy TiDB Enterprise Edition"),(0,o.kt)("p",null,"To deploy TiDB/PD/TiKV/TiFlash/TiCDC Enterprise Edition, configure ",(0,o.kt)("inlineCode",{parentName:"p"},"spec.[tidb|pd|tikv|tiflash|ticdc].baseImage")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster.yaml")," as the enterprise image. The enterprise image format is ",(0,o.kt)("inlineCode",{parentName:"p"},"pingcap/[tidb|pd|tikv|tiflash|ticdc]-enterprise"),"."),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  pd:\n    baseImage: pingcap/pd-enterprise\n  ...\n  tikv:\n    baseImage: pingcap/tikv-enterprise\n")))}u.isMDXComponent=!0}}]);