"use strict";(self.webpackChunkpingcap_docs=self.webpackChunkpingcap_docs||[]).push([[7940],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return d}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function p(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),l=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):p(p({},t),e)),a},c=function(e){var t=l(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=l(a),d=r,k=m["".concat(s,".").concat(d)]||m[d]||u[d]||o;return a?n.createElement(k,p(p({ref:t},c),{},{components:a})):n.createElement(k,p({ref:t},c))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,p=new Array(o);p[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,p[1]=i;for(var l=2;l<o;l++)p[l]=a[l];return n.createElement.apply(null,p)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8616:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return i},contentTitle:function(){return s},metadata:function(){return l},assets:function(){return c},toc:function(){return u},default:function(){return d}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),p=["components"],i={title:"Back up Data to S3-Compatible Storage Using Dumpling",summary:"Learn how to back up the TiDB cluster to the S3-compatible storage using Dumpling."},s="Back up Data to S3-Compatible Storage Using Dumpling",l={unversionedId:"backup-to-s3",id:"backup-to-s3",title:"Back up Data to S3-Compatible Storage Using Dumpling",description:'This document describes how to back up the data of the TiDB cluster in Kubernetes to an S3-compatible storage. "Backup" in this document refers to full backup (ad-hoc full backup and scheduled full backup).',source:"@site/docs/backup-to-s3.md",sourceDirName:".",slug:"/backup-to-s3",permalink:"/backup-to-s3",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/backup-to-s3.md",tags:[],version:"current",frontMatter:{title:"Back up Data to S3-Compatible Storage Using Dumpling",summary:"Learn how to back up the TiDB cluster to the S3-compatible storage using Dumpling."},sidebar:"mySidebar",previous:{title:"Restore Data from S3-Compatible Storage Using BR",permalink:"/restore-from-aws-s3-using-br"},next:{title:"Restore Data from S3-Compatible Storage Using TiDB Lightning",permalink:"/restore-from-s3"}},c={},u=[{value:"User scenarios",id:"user-scenarios",level:2},{value:"Ad-hoc full backup to S3-compatible storage",id:"ad-hoc-full-backup-to-s3-compatible-storage",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Prepare for ad-hoc full backup",id:"step-1-prepare-for-ad-hoc-full-backup",level:3},{value:"Step 2: Perform ad-hoc backup",id:"step-2-perform-ad-hoc-backup",level:3},{value:"Delete the backup CR",id:"delete-the-backup-cr",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2}],m={toc:u};function d(e){var t=e.components,a=(0,r.Z)(e,p);return(0,o.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"back-up-data-to-s3-compatible-storage-using-dumpling"},"Back up Data to S3-Compatible Storage Using Dumpling"),(0,o.kt)("p",null,'This document describes how to back up the data of the TiDB cluster in Kubernetes to an S3-compatible storage. "Backup" in this document refers to full backup (ad-hoc full backup and scheduled full backup).'),(0,o.kt)("p",null,"The backup method described in this document is implemented based on CustomResourceDefinition (CRD) in TiDB Operator v1.1 or later versions. For the underlying implementation, ",(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/dev/export-or-backup-using-dumpling"},"Dumpling")," is used to get the logic backup of the TiDB cluster, and then this backup data is sent to the S3-compatible storage."),(0,o.kt)("p",null,"Dumpling is a data export tool that exports data stored in TiDB/MySQL as SQL or CSV files and can be used to make a logical full backup or export."),(0,o.kt)("h2",{id:"user-scenarios"},"User scenarios"),(0,o.kt)("p",null,"You can use the backup method described in this document if you want to make an ",(0,o.kt)("a",{parentName:"p",href:"#ad-hoc-full-backup-to-s3-compatible-storage"},"ad-hoc full backup")," or ",(0,o.kt)("a",{parentName:"p",href:"#scheduled-full-backup-to-s3-compatible-storage"},"scheduled full backup")," of the TiDB cluster data to S3-compatible storages with the following needs:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"To export SQL or CSV files"),(0,o.kt)("li",{parentName:"ul"},"To limit the memory usage of a single SQL statement"),(0,o.kt)("li",{parentName:"ul"},"To export the historical data snapshot of TiDB")),(0,o.kt)("h2",{id:"ad-hoc-full-backup-to-s3-compatible-storage"},"Ad-hoc full backup to S3-compatible storage"),(0,o.kt)("p",null,"Ad-hoc full backup describes the backup by creating a ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," custom resource (CR) object. TiDB Operator performs the specific backup operation based on this ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," object. If an error occurs during the backup process, TiDB Operator does not retry and you need to handle this error manually."),(0,o.kt)("p",null,"For the current S3-compatible storage types, Ceph and Amazon S3 work normally as tested. Therefore, this document shows examples in which the data of the ",(0,o.kt)("inlineCode",{parentName:"p"},"demo1")," TiDB cluster in the ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster")," Kubernetes namespace is backed up to Ceph and Amazon S3 respectively."),(0,o.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("p",null,"Before you use Dumpling to back up the TiDB cluster data to the S3-compatible storage, make sure that you have the following privileges:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The ",(0,o.kt)("inlineCode",{parentName:"li"},"SELECT")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"UPDATE")," privileges of the ",(0,o.kt)("inlineCode",{parentName:"li"},"mysql.tidb")," table: Before and after the backup, the ",(0,o.kt)("inlineCode",{parentName:"li"},"Backup")," CR needs a database account with these privileges to adjust the GC time."),(0,o.kt)("li",{parentName:"ul"},"The global privileges: ",(0,o.kt)("inlineCode",{parentName:"li"},"SELECT"),", ",(0,o.kt)("inlineCode",{parentName:"li"},"RELOAD"),", ",(0,o.kt)("inlineCode",{parentName:"li"},"LOCK TABLES")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"REPLICATION CLIENT"))),(0,o.kt)("p",null,"An example for creating a backup user:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE USER 'backup'@'%' IDENTIFIED BY '...';\nGRANT\n  SELECT, RELOAD, LOCK TABLES, REPLICATION CLIENT\n  ON *.*\n  TO 'backup'@'%';\nGRANT\n  UPDATE, SELECT\n  ON mysql.tidb\n  TO 'backup'@'%';\n")),(0,o.kt)("h3",{id:"step-1-prepare-for-ad-hoc-full-backup"},"Step 1: Prepare for ad-hoc full backup"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Execute the following command to create the role-based access control (RBAC) resources in the ",(0,o.kt)("inlineCode",{parentName:"p"},"tidb-cluster")," namespace based on ",(0,o.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/backup/backup-rbac.yaml"},"backup-rbac.yaml"),":"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/backup/backup-rbac.yaml -n tidb-cluster\n"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Grant permissions to the remote storage."),(0,o.kt)("p",{parentName:"li"},"To grant permissions to access S3-compatible remote storage, refer to ",(0,o.kt)("a",{parentName:"p",href:"/grant-permissions-to-remote-storage#aws-account-permissions"},"AWS account permissions"),"."),(0,o.kt)("p",{parentName:"li"},"If you use Ceph as the backend storage for testing, you can grant permissions by ",(0,o.kt)("a",{parentName:"p",href:"/grant-permissions-to-remote-storage#grant-permissions-by-accesskey-and-secretkey"},"using AccessKey and SecretKey"),".")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-demo1-tidb-secret")," secret which stores the root account and password needed to access the TiDB cluster:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl create secret generic backup-demo1-tidb-secret --from-literal=password=${password} --namespace=tidb-cluster\n")))),(0,o.kt)("h3",{id:"step-2-perform-ad-hoc-backup"},"Step 2: Perform ad-hoc backup"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Because of the ",(0,o.kt)("inlineCode",{parentName:"p"},"rclone")," ",(0,o.kt)("a",{parentName:"p",href:"https://rclone.org/s3/#key-management-system-kms"},"issue"),", if the backup data is stored in Amazon S3 and the ",(0,o.kt)("inlineCode",{parentName:"p"},"AWS-KMS")," encryption is enabled, you need to add the following ",(0,o.kt)("inlineCode",{parentName:"p"},"spec.s3.options")," configuration to the YAML file in the examples of this section:"),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  s3:\n    ...\n    options:\n    - --ignore-checksum\n")),(0,o.kt)("p",{parentName:"div"},"This section lists multiple storage access methods. Only follow the method that matches your situation. The methods are as follows:"),(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},"Amazon S3 by importing AccessKey and SecretKey"),(0,o.kt)("li",{parentName:"ul"},"Ceph by importing AccessKey and SecretKey"),(0,o.kt)("li",{parentName:"ul"},"Amazon S3 by binding IAM with Pod"),(0,o.kt)("li",{parentName:"ul"},"Amazon S3 by binding IAM with ServiceAccount")),(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 1: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR, and back up cluster data to Amazon S3 by importing AccessKey and SecretKey to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: Backup\nmetadata:\n  name: demo1-backup-s3\n  namespace: tidb-cluster\nspec:\n  from:\n    host: ${tidb_host}\n    port: ${tidb_port}\n    user: ${tidb_user}\n    secretName: backup-demo1-tidb-secret\n  s3:\n    provider: aws\n    secretName: s3-secret\n    region: ${region}\n    bucket: ${bucket}\n    # prefix: ${prefix}\n    # storageClass: STANDARD_IA\n    # acl: private\n    # endpoint:\n# dumpling:\n#  options:\n#  - --threads=16\n#  - --rows=10000\n#  tableFilter:\n#  - "test.*"\n  # storageClassName: local-storage\n  storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 2: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR, and back up data to Ceph by importing AccessKey and SecretKey to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: Backup\nmetadata:\n  name: demo1-backup-s3\n  namespace: tidb-cluster\nspec:\n  from:\n    host: ${tidb_host}\n    port: ${tidb_port}\n    user: ${tidb_user}\n    secretName: backup-demo1-tidb-secret\n  s3:\n    provider: ceph\n    secretName: s3-secret\n    endpoint: ${endpoint}\n    # prefix: ${prefix}\n    bucket: ${bucket}\n# dumpling:\n#  options:\n#  - --threads=16\n#  - --rows=10000\n#  tableFilter:\n#  - "test.*"\n  # storageClassName: local-storage\n  storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 3: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR, and back up data to Amazon S3 by binding IAM with Pod to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: Backup\nmetadata:\nname: demo1-backup-s3\nnamespace: tidb-cluster\nannotations:\n    iam.amazonaws.com/role: arn:aws:iam::123456789012:role/user\nspec:\nbackupType: full\nfrom:\n    host: ${tidb_host}\n    port: ${tidb_port}\n    user: ${tidb_user}\n    secretName: backup-demo1-tidb-secret\ns3:\n    provider: aws\n    region: ${region}\n    bucket: ${bucket}\n    # prefix: ${prefix}\n    # storageClass: STANDARD_IA\n    # acl: private\n    # endpoint:\n# dumpling:\n#  options:\n#  - --threads=16\n#  - --rows=10000\n#  tableFilter:\n#  - "test.*"\n  # storageClassName: local-storage\n  storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 4: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR, and back up data to Amazon S3 by binding IAM with ServiceAccount to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"kubectl apply -f backup-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: Backup\nmetadata:\nname: demo1-backup-s3\nnamespace: tidb-cluster\nspec:\nbackupType: full\nserviceAccount: tidb-backup-manager\nfrom:\n    host: ${tidb_host}\n    port: ${tidb_port}\n    user: ${tidb_user}\n    secretName: backup-demo1-tidb-secret\ns3:\n    provider: aws\n    region: ${region}\n    bucket: ${bucket}\n    # prefix: ${prefix}\n    # storageClass: STANDARD_IA\n    # acl: private\n    # endpoint:\n# dumpling:\n#  options:\n#  - --threads=16\n#  - --rows=10000\n#  tableFilter:\n#  - "test.*"\n  # storageClassName: local-storage\n  storageSize: 10Gi\n')))),(0,o.kt)("p",{parentName:"div"},"In the examples above, all data of the TiDB cluster is exported and backed up to Amazon S3 or Ceph. You can ignore the ",(0,o.kt)("inlineCode",{parentName:"p"},"acl"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"endpoint"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"storageClass")," fields in the Amazon S3 configuration. Other S3-compatible storages can also use a configuration similar to that of Amazon S3. You can also leave the fields empty if you do not need to configure them, as shown in the above Ceph configuration. For more information about S3-compatible storage configuration, refer to ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#s3-storage-fields"},"S3 storage fields"),"."),(0,o.kt)("p",{parentName:"div"},(0,o.kt)("inlineCode",{parentName:"p"},"spec.dumpling")," refers to Dumpling-related configuration. You can specify Dumpling's operation parameters in the ",(0,o.kt)("inlineCode",{parentName:"p"},"options")," field. See ",(0,o.kt)("a",{parentName:"p",href:"https://docs.pingcap.com/tidb/stable/dumpling-overview#option-list-of-dumpling"},"Dumpling Option list")," for more information. These configuration items of Dumpling can be ignored by default. When these items are not specified, the default values of ",(0,o.kt)("inlineCode",{parentName:"p"},"options")," fields are as follows:"),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre"},"options:\n- --threads=16\n- --rows=10000\n")),(0,o.kt)("p",{parentName:"div"},"For more information about the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR fields, refer to ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#backup-cr-fields"},"Backup CR fields"),"."),(0,o.kt)("p",{parentName:"div"},"After creating the ",(0,o.kt)("inlineCode",{parentName:"p"},"Backup")," CR, use the following command to check the backup status:"),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get bk -n tidb-cluster -owide\n")),(0,o.kt)("p",{parentName:"div"},"To get detailed information on a backup job, use the following command. For ",(0,o.kt)("inlineCode",{parentName:"p"},"$backup_job_name")," in the command, use the name from the output of the previous command."),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl describe bk -n tidb-cluster $backup_job_name\n")),(0,o.kt)("p",{parentName:"div"},"To run ad-hoc backup again, you need to ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#delete-the-backup-cr"},"delete the backup CR")," and create it again."),(0,o.kt)("h2",{parentName:"div",id:"scheduled-full-backup-to-s3-compatible-storage"},"Scheduled full backup to S3-compatible storage"),(0,o.kt)("p",{parentName:"div"},"You can set a backup policy to perform scheduled backups of the TiDB cluster, and set a backup retention policy to avoid excessive backup items. A scheduled full backup is described by a custom ",(0,o.kt)("inlineCode",{parentName:"p"},"BackupSchedule")," CR object. A full backup is triggered at each backup time point. Its underlying implementation is the ad-hoc full backup."),(0,o.kt)("h3",{parentName:"div",id:"step-1-prepare-for-scheduled-backup"},"Step 1: Prepare for scheduled backup"),(0,o.kt)("p",{parentName:"div"},"The prerequisites for the scheduled backup is the same as the ",(0,o.kt)("a",{parentName:"p",href:"#step-1-prepare-for-ad-hoc-full-backup"},"prepare for ad-hoc full backup"),"."),(0,o.kt)("h3",{parentName:"div",id:"step-2-perform-scheduled-backup"},"Step 2: Perform scheduled backup"))),(0,o.kt)("p",null,"Because of the ",(0,o.kt)("inlineCode",{parentName:"p"},"rclone")," ",(0,o.kt)("a",{parentName:"p",href:"https://rclone.org/s3/#key-management-system-kms"},"issue"),", if the backup data is stored in Amazon S3 and the ",(0,o.kt)("inlineCode",{parentName:"p"},"AWS-KMS")," encryption is enabled, you need to add the following ",(0,o.kt)("inlineCode",{parentName:"p"},"spec.backupTemplate.s3.options")," configuration to the YAML file in the examples of this section:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  ...\n  backupTemplate:\n    ...\n    s3:\n      ...\n      options:\n      - --ignore-checksum\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 1: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"BackupSchedule")," CR to enable the scheduled full backup to Amazon S3 by importing AccessKey and SecretKey to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-schedule-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-schedule-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: BackupSchedule\nmetadata:\n  name: demo1-backup-schedule-s3\n  namespace: tidb-cluster\nspec:\n  #maxBackups: 5\n  #pause: true\n  maxReservedTime: "3h"\n  schedule: "*/2 * * * *"\n  backupTemplate:\n    from:\n      host: ${tidb_host}\n      port: ${tidb_port}\n      user: ${tidb_user}\n      secretName: backup-demo1-tidb-secret\n    s3:\n      provider: aws\n      secretName: s3-secret\n      region: ${region}\n      bucket: ${bucket}\n      # prefix: ${prefix}\n      # storageClass: STANDARD_IA\n      # acl: private\n      # endpoint:\n  # dumpling:\n  #  options:\n  #  - --threads=16\n  #  - --rows=10000\n  #  tableFilter:\n  #  - "test.*"\n    # storageClassName: local-storage\n    storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 2: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"BackupSchedule")," CR to enable the scheduled full backup to Ceph by importing AccessKey and SecretKey to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-schedule-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-schedule-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: BackupSchedule\nmetadata:\n  name: demo1-backup-schedule-ceph\n  namespace: tidb-cluster\nspec:\n  #maxBackups: 5\n  #pause: true\n  maxReservedTime: "3h"\n  schedule: "*/2 * * * *"\n  backupTemplate:\n    from:\n      host: ${tidb_host}\n      port: ${tidb_port}\n      user: ${tidb_user}\n      secretName: backup-demo1-tidb-secret\n    s3:\n      provider: ceph\n      secretName: s3-secret\n      endpoint: ${endpoint}\n      bucket: ${bucket}\n      # prefix: ${prefix}\n  # dumpling:\n  #  options:\n  #  - --threads=16\n  #  - --rows=10000\n  #  tableFilter:\n  #  - "test.*"\n    # storageClassName: local-storage\n    storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 3: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"BackupSchedule")," CR to enable the scheduled full backup, and back up the cluster data to Amazon S3 by binding IAM with Pod to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-schedule-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-schedule-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: BackupSchedule\nmetadata:\n  name: demo1-backup-schedule-s3\n  namespace: tidb-cluster\n  annotations:\n    iam.amazonaws.com/role: arn:aws:iam::123456789012:role/user\nspec:\n  #maxBackups: 5\n  #pause: true\n  maxReservedTime: "3h"\n  schedule: "*/2 * * * *"\n  backupTemplate:\n    from:\n      host: ${tidb_host}\n      port: ${tidb_port}\n      user: ${tidb_user}\n      secretName: backup-demo1-tidb-secret\n    s3:\n      provider: aws\n      region: ${region}\n      bucket: ${bucket}\n      # prefix: ${prefix}\n      # storageClass: STANDARD_IA\n      # acl: private\n      # endpoint:\n  # dumpling:\n  #  options:\n  #  - --threads=16\n  #  - --rows=10000\n  #  tableFilter:\n  #  - "test.*"\n    # storageClassName: local-storage\n    storageSize: 10Gi\n'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Method 4: Create the ",(0,o.kt)("inlineCode",{parentName:"p"},"BackupSchedule")," CR to enable the scheduled full backup, and back up the cluster data to Amazon S3 by binding IAM with ServiceAccount to grant permissions:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl apply -f backup-schedule-s3.yaml\n")),(0,o.kt)("p",{parentName:"li"},"  The content of ",(0,o.kt)("inlineCode",{parentName:"p"},"backup-schedule-s3.yaml")," is as follows:"),(0,o.kt)("pre",{parentName:"li"},(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\napiVersion: pingcap.com/v1alpha1\nkind: BackupSchedule\nmetadata:\n  name: demo1-backup-schedule-s3\n  namespace: tidb-cluster\nspec:\n  #maxBackups: 5\n  #pause: true\n  maxReservedTime: "3h"\n  schedule: "*/2 * * * *"\n  serviceAccount: tidb-backup-manager\n  backupTemplate:\n    from:\n      host: ${tidb_host}\n      port: ${tidb_port}\n      user: ${tidb_user}\n      secretName: backup-demo1-tidb-secret\n    s3:\n      provider: aws\n      region: ${region}\n      bucket: ${bucket}\n      # prefix: ${prefix}\n      # storageClass: STANDARD_IA\n      # acl: private\n      # endpoint:\n  # dumpling:\n  #  options:\n  #  - --threads=16\n  #  - --rows=10000\n  #  tableFilter:\n  #  - "test.*"\n    # storageClassName: local-storage\n    storageSize: 10Gi\n')))),(0,o.kt)("p",null,"After creating the scheduled full backup, you can use the following command to check the backup status:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get bks -n tidb-cluster -owide\n")),(0,o.kt)("p",null,"You can use the following command to check all the backup items:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl get bk -l tidb.pingcap.com/backup-schedule=demo1-backup-schedule-s3 -n tidb-cluster\n")),(0,o.kt)("p",null,"From the example above, you can see that the ",(0,o.kt)("inlineCode",{parentName:"p"},"backupSchedule")," configuration consists of two parts. One is the unique configuration of ",(0,o.kt)("inlineCode",{parentName:"p"},"backupSchedule"),", and the other is ",(0,o.kt)("inlineCode",{parentName:"p"},"backupTemplate"),"."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"backupTemplate")," specifies the configuration related to the cluster and remote storage, which is the same as the ",(0,o.kt)("inlineCode",{parentName:"p"},"spec")," configuration of ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#backup-cr-fields"},"the ",(0,o.kt)("inlineCode",{parentName:"a"},"Backup")," CR"),". For the unique configuration of ",(0,o.kt)("inlineCode",{parentName:"p"},"backupSchedule"),", refer to ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#backupschedule-cr-fields"},"BackupSchedule CR fields"),"."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"TiDB Operator creates a PVC used for both ad-hoc full backup and scheduled full backup. The backup data is stored in PV first and then uploaded to remote storage. If you want to delete this PVC after the backup is completed, you can refer to ",(0,o.kt)("a",{parentName:"p",href:"/cheat-sheet#delete-resources"},"Delete Resource")," to delete the backup Pod first, and then delete the PVC.\nIf the backup data is successfully uploaded to remote storage, TiDB Operator automatically deletes the local data. If the upload fails, the local data is retained."))),(0,o.kt)("h2",{id:"delete-the-backup-cr"},"Delete the backup CR"),(0,o.kt)("p",null,"After the backup, you might need to delete the backup CR. For details, refer to ",(0,o.kt)("a",{parentName:"p",href:"/backup-restore-overview#delete-the-backup-cr"},"Delete the Backup CR"),"."),(0,o.kt)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,o.kt)("p",null,"If you encounter any problem during the backup process, refer to ",(0,o.kt)("a",{parentName:"p",href:"/deploy-failures"},"Common Deployment Failures"),"."))}d.isMDXComponent=!0}}]);